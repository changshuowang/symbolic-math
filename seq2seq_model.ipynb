{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start from baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-addons\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "print(tf.__version__)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import urllib3\n",
    "import shutil\n",
    "import zipfile\n",
    "import itertools\n",
    "#from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"./data/bwd_sample5000.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [equation, integral]\n",
    "def create_dataset(path, num_examples=None):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> * -1 * pow x -1 * pow + 1 x -1 + * 4 * x + 1 x * 2 * pow * x + 1 x / 1 2 * pow atan 5 / 1 2 * + 1 * 2 x sinh * pow * x + 1 x / 1 2 pow atan 5 / 1 2 <end>\n",
      "<start> + * -4 x * -4 cosh * pow * x + 1 x / 1 2 pow atan 5 / 1 2 <end>\n"
     ]
    }
   ],
   "source": [
    "eq, intgr = create_dataset(path_to_file)\n",
    "print(eq[-1])\n",
    "print(intgr[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text, Y_text = create_dataset(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to tokenize words into index using inbuild tokenizer vocabulory\n",
    "def tokenize(inp):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(inp)\n",
    "    sequences = tokenizer.texts_to_sequences(inp)\n",
    "    # print(max_len(sequences))\n",
    "    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post',  maxlen=512, truncating='post')\n",
    "    return  sequences, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len(tensor):\n",
    "    #print( np.argmax([len(t) for t in tensor]))\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each word into index and return the tokenized list and tokenizer\n",
    "X , X_tokenizer = tokenize(X_text)\n",
    "Y, Y_tokenizer = tokenize(Y_text)\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "Tx = max_len(X)\n",
    "Ty = max_len(Y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3160, 512)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length  equation sentence denoted as Tx :  512\n",
      "Max length intergral sentence denoted as Ty:  512\n"
     ]
    }
   ],
   "source": [
    "print(\"Max length  equation sentence denoted as Tx : \", Tx)\n",
    "print(\"Max length intergral sentence denoted as Ty: \", Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size :  301\n",
      "output_vocab_size :  228\n"
     ]
    }
   ],
   "source": [
    "X_tokenizer.word_index['<start>'] #'<start>': 2   # tokenize by frequency\n",
    "input_vocab_size = len(X_tokenizer.word_index)+1  # add 1 for 0 sequence character\n",
    "output_vocab_size = len(Y_tokenizer.word_index)+ 1\n",
    "print(\"input_vocab_size : \", input_vocab_size)\n",
    "print(\"output_vocab_size : \" ,output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'*': 1,\n",
       " '+': 2,\n",
       " 'pow': 3,\n",
       " 'x': 4,\n",
       " '-1': 5,\n",
       " '2': 6,\n",
       " '/': 7,\n",
       " '1': 8,\n",
       " '<start>': 9,\n",
       " '<end>': 10,\n",
       " '3': 11,\n",
       " '4': 12,\n",
       " '-2': 13,\n",
       " '5': 14,\n",
       " 'cos': 15,\n",
       " 'cosh': 16,\n",
       " 'sinh': 17,\n",
       " 'sin': 18,\n",
       " 'exp': 19,\n",
       " 'tanh': 20,\n",
       " 'tan': 21,\n",
       " 'atan': 22,\n",
       " 'atanh': 23,\n",
       " 'asin': 24,\n",
       " '-3': 25,\n",
       " 'asinh': 26,\n",
       " 'acosh': 27,\n",
       " 'acos': 28,\n",
       " '-4': 29,\n",
       " 'log': 30,\n",
       " '-5': 31,\n",
       " '6': 32,\n",
       " '8': 33,\n",
       " 'pi': 34,\n",
       " '9': 35,\n",
       " '10': 36,\n",
       " '12': 37,\n",
       " '16': 38,\n",
       " '15': 39,\n",
       " '7': 40,\n",
       " '25': 41,\n",
       " '20': 42,\n",
       " '0': 43,\n",
       " '-9': 44,\n",
       " '-6': 45,\n",
       " '-10': 46,\n",
       " '-8': 47,\n",
       " '-12': 48,\n",
       " '-7': 49,\n",
       " '-16': 50,\n",
       " '-25': 51,\n",
       " '-20': 52,\n",
       " '-15': 53,\n",
       " '24': 54,\n",
       " '18': 55,\n",
       " '13': 56,\n",
       " '11': 57,\n",
       " '36': 58,\n",
       " '40': 59,\n",
       " '-24': 60,\n",
       " '30': 61,\n",
       " 'abs': 62,\n",
       " '14': 63,\n",
       " '64': 64,\n",
       " '32': 65,\n",
       " '100': 66,\n",
       " '21': 67,\n",
       " '-40': 68,\n",
       " '60': 69,\n",
       " '-13': 70,\n",
       " '-18': 71,\n",
       " '17': 72,\n",
       " '48': 73,\n",
       " '-14': 74,\n",
       " '-36': 75,\n",
       " '50': 76,\n",
       " '26': 77,\n",
       " '-32': 78,\n",
       " '33': 79,\n",
       " '-64': 80,\n",
       " '-48': 81,\n",
       " '80': 82,\n",
       " '22': 83,\n",
       " '23': 84,\n",
       " '-100': 85,\n",
       " '-30': 86,\n",
       " '-21': 87,\n",
       " '45': 88,\n",
       " '75': 89,\n",
       " '27': 90,\n",
       " '225': 91,\n",
       " '-19': 92,\n",
       " '-400': 93,\n",
       " '125': 94,\n",
       " '-35': 95,\n",
       " '-17': 96,\n",
       " '-144': 97,\n",
       " '-33': 98,\n",
       " '-27': 99,\n",
       " '35': 100,\n",
       " '144': 101,\n",
       " '49': 102,\n",
       " '-80': 103,\n",
       " '120': 104,\n",
       " '150': 105,\n",
       " '19': 106,\n",
       " '29': 107,\n",
       " '39': 108,\n",
       " '96': 109,\n",
       " '-11': 110,\n",
       " '-45': 111,\n",
       " '28': 112,\n",
       " '625': 113,\n",
       " '76': 114,\n",
       " '-96': 115,\n",
       " '1024': 116,\n",
       " '-23': 117,\n",
       " '-50': 118,\n",
       " '-60': 119,\n",
       " '81': 120,\n",
       " '53': 121,\n",
       " '240': 122,\n",
       " '-75': 123,\n",
       " '-81': 124,\n",
       " '243': 125,\n",
       " '160': 126,\n",
       " '-225': 127,\n",
       " '200': 128,\n",
       " '41': 129,\n",
       " '-160': 130,\n",
       " '52': 131,\n",
       " '63': 132,\n",
       " '-150': 133,\n",
       " '128': 134,\n",
       " '-26': 135,\n",
       " '-29': 136,\n",
       " '66': 137,\n",
       " '-28': 138,\n",
       " '-256': 139,\n",
       " '-39': 140,\n",
       " '256': 141,\n",
       " '-49': 142,\n",
       " '-125': 143,\n",
       " '85': 144,\n",
       " '-5625': 145,\n",
       " '34': 146,\n",
       " '300': 147,\n",
       " '38': 148,\n",
       " '-108': 149,\n",
       " '57': 150,\n",
       " '288': 151,\n",
       " '900': 152,\n",
       " '108': 153,\n",
       " '400': 154,\n",
       " '-34': 155,\n",
       " '408': 156,\n",
       " '56': 157,\n",
       " '-68': 158,\n",
       " '1296': 159,\n",
       " '99': 160,\n",
       " '73': 161,\n",
       " '77': 162,\n",
       " '42': 163,\n",
       " '432': 164,\n",
       " '320': 165,\n",
       " '324': 166,\n",
       " '540': 167,\n",
       " '-63': 168,\n",
       " '401': 169,\n",
       " '-320': 170,\n",
       " '298023223876953125': 171,\n",
       " '-51': 172,\n",
       " '252': 173,\n",
       " '1200': 174,\n",
       " '251': 175,\n",
       " '101': 176,\n",
       " '-900': 177,\n",
       " '72': 178,\n",
       " '180': 179,\n",
       " '70': 180,\n",
       " '-22': 181,\n",
       " '103': 182,\n",
       " '840': 183,\n",
       " '-115': 184,\n",
       " '115': 185,\n",
       " '172': 186,\n",
       " 'sign(x)': 187,\n",
       " '-42': 188,\n",
       " '-110': 189,\n",
       " '-2640': 190,\n",
       " '960': 191,\n",
       " '1073741824': 192,\n",
       " '500': 193,\n",
       " '675': 194,\n",
       " '-720': 195,\n",
       " '282': 196,\n",
       " '17293': 197,\n",
       " '-45342': 198,\n",
       " '-18028': 199,\n",
       " '3200': 200,\n",
       " '7200': 201,\n",
       " '17760': 202,\n",
       " '19320': 203,\n",
       " '74': 204,\n",
       " '-66': 205,\n",
       " '-384': 206,\n",
       " '1536': 207,\n",
       " '9184': 208,\n",
       " '-55': 209,\n",
       " '186624': 210,\n",
       " '18045': 211,\n",
       " '1125': 212,\n",
       " '9000': 213,\n",
       " '192': 214,\n",
       " '262144': 215,\n",
       " '-512': 216,\n",
       " '-250': 217,\n",
       " '-624': 218,\n",
       " '-180': 219,\n",
       " '626': 220,\n",
       " '-270': 221,\n",
       " '-1225': 222,\n",
       " '600': 223,\n",
       " '-46': 224,\n",
       " '-640': 225,\n",
       " '149': 226,\n",
       " '210': 227,\n",
       " '-245': 228,\n",
       " '-360': 229,\n",
       " '-336': 230,\n",
       " '340': 231,\n",
       " '-540': 232,\n",
       " '195': 233,\n",
       " '-11200': 234,\n",
       " '160000': 235,\n",
       " '-500': 236,\n",
       " '121': 237,\n",
       " '550': 238,\n",
       " '-120': 239,\n",
       " '-22500': 240,\n",
       " '90': 241,\n",
       " '-8400': 242,\n",
       " '-6000': 243,\n",
       " '-3840': 244,\n",
       " '-516': 245,\n",
       " '-38': 246,\n",
       " '-1600': 247,\n",
       " '-56': 248,\n",
       " '196': 249,\n",
       " '3125': 250,\n",
       " '384': 251,\n",
       " '6912': 252,\n",
       " '55296': 253,\n",
       " '165888': 254,\n",
       " '-2592': 255,\n",
       " '-432': 256,\n",
       " '-6912': 257,\n",
       " '-1728': 258,\n",
       " '-195': 259,\n",
       " '-90': 260,\n",
       " '-576': 261,\n",
       " '162': 262,\n",
       " '450': 263,\n",
       " '-99': 264,\n",
       " '84': 265,\n",
       " '47': 266,\n",
       " '71': 267,\n",
       " '-168': 268,\n",
       " '365': 269,\n",
       " '194': 270,\n",
       " 'accumbounds(-1,': 271,\n",
       " '1)': 272,\n",
       " '-732': 273,\n",
       " '480': 274,\n",
       " '1201': 275,\n",
       " '-200': 276,\n",
       " '-441': 277,\n",
       " '-300': 278,\n",
       " '1599': 279,\n",
       " '-800': 280,\n",
       " '299': 281,\n",
       " '-301': 282,\n",
       " '51': 283,\n",
       " '22500': 284,\n",
       " '4100625': 285,\n",
       " '-375': 286,\n",
       " '-1000': 287,\n",
       " '-37': 288,\n",
       " '82': 289,\n",
       " '67': 290,\n",
       " 'sign(sin(x))': 291,\n",
       " '-287': 292,\n",
       " '-44': 293,\n",
       " '-609': 294,\n",
       " '2209': 295,\n",
       " '-94': 296,\n",
       " '-220': 297,\n",
       " '576': 298,\n",
       " '-6400': 299,\n",
       " '37': 300}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = len(X_train)\n",
    "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dims = 128\n",
    "rnn_units = 256\n",
    "dense_units = 256\n",
    "Dtype = tf.float32   #used to initialize DecoderCell Zero state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "example_X, example_Y = next(iter(dataset))\n",
    "print(example_X.shape) \n",
    "print(example_Y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "example_X, example_Y = next(iter(dataset))\n",
    "print(example_X.shape) \n",
    "print(example_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Encoder-Decoder Model based on tfa.seq2seq module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model\n",
    "\n",
    "The encoder network consists of an encoder embedding layer and a LSTM layer.\n",
    "\n",
    "The decoder network encompasses both decoder and attention mechanism.\n",
    "\n",
    "The attention uses LuongAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODER\n",
    "class EncoderNetwork(tf.keras.Model):\n",
    "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
    "                                                           output_dim=embedding_dims)\n",
    "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
    "                                                     return_state=True )\n",
    "    \n",
    "#DECODER\n",
    "class DecoderNetwork(tf.keras.Model):\n",
    "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
    "        super().__init__()\n",
    "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
    "                                                           output_dim=embedding_dims) \n",
    "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
    "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
    "        # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        # Create attention mechanism with memory = None\n",
    "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
    "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
    "                                                output_layer=self.dense_layer)\n",
    "\n",
    "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
    "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
    "                                          memory_sequence_length=memory_sequence_length)\n",
    "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "    # wrap decodernn cell  \n",
    "    def build_rnn_cell(self, batch_size ):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
    "                                                attention_layer_size=dense_units)\n",
    "        return rnn_cell\n",
    "    \n",
    "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
    "                                                                dtype = Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
    "        return decoder_initial_state\n",
    "\n",
    "\n",
    "\n",
    "encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
    "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second way of define encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True\n",
    "                                    )\n",
    "\n",
    "  def __call__(self, x, initial_state):\n",
    "    x = self.embedding(x)\n",
    "    output, state_h, state_cell = self.lstm(x, initial_state = initial_state)\n",
    "    return output, state_h, state_cell\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return  [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_vocab_size, embedding_dims, rnn_units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "initial_state = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_X, initial_state)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_h.shape))\n",
    "print ('Encoder cell state shape: (batch size, units) {}'.format(sample_c.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def __call__(self, query, values):\n",
    "    # hidden shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # we are doing this to perform addition to calculate the score\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_h, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_h, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.expand_dims(sample_h, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   )\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def __call__(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "     \n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the lstm\n",
    "    output, state_h, state_c = self.lstm(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state_h, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(output_vocab_size, embedding_dims, rnn_units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_h, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, mask is a zero-one matrix of the same size as decoder_outputs. It masks padding positions outside of the target sequence lengths with values 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_pred, y):\n",
    "   \n",
    "    #shape of y [batch_size, ty]\n",
    "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
    "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                                                  reduction='none')\n",
    "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
    "    #skip loss calculation for padding sequences i.e. y = 0 \n",
    "    #[ <start>,How, are, you, today, 0, 0, 0, 0 ....<end>]\n",
    "    #[ 1, 234, 3, 423, 3344, 0, 0 ,0 ,0, 2 ]\n",
    "    # y is a tensor of [batch_size,Ty] . Create a mask when [y=0]\n",
    "    # mask the loss when padding sequence appears in the output sequence\n",
    "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, attention mechanism is initialized without memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoderNetwork.attention_mechanism.memory_initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
    "    #initialize loss = 0\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
    "        #print(encoder_emb_inp.shape)\n",
    "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
    "                                                        initial_state =encoder_initial_cell_state)\n",
    "        #print(a.shape)\n",
    "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
    "        \n",
    "\n",
    "         \n",
    "        # Prepare correct Decoder input & output sequence data\n",
    "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
    "        #compare logits with timestepped +1 version of decoder_input\n",
    "        decoder_output = output_batch[:,1:] #ignore <start>\n",
    "\n",
    "\n",
    "        # Decoder Embeddings\n",
    "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
    "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
    "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
    "                                                                           encoder_state=[a_tx, c_tx],\n",
    "                                                                           Dtype=tf.float32)\n",
    "\n",
    "        #BasicDecoderOutput        \n",
    "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
    "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
    "\n",
    "        logits = outputs.rnn_output\n",
    "        #Calculate loss\n",
    "\n",
    "        loss = loss_function(logits, decoder_output)\n",
    "\n",
    "    #Returns the list of all layer variables / weights.\n",
    "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
    "    # differentiate loss wrt variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    #grads_and_vars – List of(gradient, variable) pairs.\n",
    "    grads_and_vars = zip(gradients,variables)\n",
    "    optimizer.apply_gradients(grads_and_vars)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN LSTM hidden and memory state initializer\n",
    "def initialize_initial_state():\n",
    "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "from collections import defaultdict\n",
    "data = defaultdict(list)\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "\n",
    "    encoder_initial_cell_state = initialize_initial_state()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        #encoder_initial_cell_state = initialize_initial_state() # TODO\n",
    "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
    "        total_loss += batch_loss\n",
    "#        if (batch+1)%20 == 0:\n",
    "        print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))\n",
    "            #checkpoint.save(file_prefix = chkpoint_prefix)\n",
    "        data['batch'].append(total_loss)\n",
    "    data['epoch'].append(total_loss)\n",
    "    #print(data['epoch'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(data['batch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total loss: 0.23502042889595032 epoch 40 batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_initial_cell_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE*[Tx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "Create input sequence to pass to encoder.\n",
    "\n",
    "The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "\n",
    "Stop predicting when the model predicts the end token.\n",
    "\n",
    "And store the attention weights for every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if trained in same session else use checkpoint variable\n",
    "#decoder_embedding_matrix = tf.train.load_variable(checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
    "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
    "print(decoderNetwork.decoder_embedding.variables[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if restoring from checkpoint, lets print all variables related to decoder_embeddings and then select and load the right variable containing decoder embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [print(var) for var in tf.train.list_variables(\n",
    "#     checkpointdir) if re.match(r'.*decoder_embedding.*',var[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_embedding_matrix = tf.train.load_variable(\n",
    "#     checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
    "# print(decoder_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use with scope /cpu:0 for inferencing\n",
    "#restore from latest checkpoint for inferencing\n",
    "input_raw=\"1  \\nx\"\n",
    "#input_raw=\"Wow!\"  #checking translation on training set record\n",
    "#def inference(input_raw):\n",
    "input_lines = input_raw.split(\"\\n\")\n",
    "\n",
    "# Preprocess X\n",
    "input_lines = [preprocess_sentence(line) for line in input_lines]\n",
    "input_sequences = [[X_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
    "                                                                maxlen=Tx, padding='post')\n",
    "inp = tf.convert_to_tensor(input_sequences)\n",
    "#print(inp.shape)\n",
    "inference_batch_size = input_sequences.shape[0]\n",
    "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
    "                              tf.zeros((inference_batch_size, rnn_units))]\n",
    "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
    "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
    "                                                initial_state =encoder_initial_cell_state)\n",
    "\n",
    "\n",
    "#output_sequences = []\n",
    "print('a_tx :',a_tx.shape)\n",
    "print('c_tx :', c_tx.shape)\n",
    "\n",
    "\n",
    "\n",
    "start_tokens = tf.fill([inference_batch_size],Y_tokenizer.word_index['<start>'])\n",
    "#print(start_tokens)\n",
    "end_token = Y_tokenizer.word_index['<end>']\n",
    "\n",
    "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "#finished,start_inputs = greedy_sampler.initialize(decoder_embedding_matrix,start_tokens,end_token)\n",
    "#print(finished.shape, start_inputs.shape)\n",
    "\n",
    "decoder_input = tf.expand_dims([Y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
    "\n",
    "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
    "                                            output_layer=decoderNetwork.dense_layer)\n",
    "decoderNetwork.attention_mechanism.setup_memory(a)\n",
    "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
    "print(\"decoder_initial_state = [a_tx, c_tx] :\",np.array([a_tx, c_tx]).shape)\n",
    "decoder_initial_state = decoderNetwork.build_decoder_initial_state(inference_batch_size,\n",
    "                                                                   encoder_state=[a_tx, c_tx],\n",
    "                                                                   Dtype=tf.float32)\n",
    "print(\"\\nCompared to simple encoder-decoder without attention, the decoder_initial_state \\\n",
    " is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \\n \")\n",
    "print(\"decoder initial state shape :\",np.array(decoder_initial_state).shape)\n",
    "print(\"decoder_initial_state tensor \\n\", decoder_initial_state)\n",
    "\n",
    "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
    "# One heuristic is to decode up to two times the source sentence lengths.\n",
    "maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
    "\n",
    "#initialize inference decoder\n",
    "\n",
    "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
    "                             start_tokens = start_tokens,\n",
    "                             end_token=end_token,\n",
    "                             initial_state = decoder_initial_state)\n",
    "#print( first_finished.shape)\n",
    "print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
    "print(\"start_index_emb_avg \", tf.reduce_sum(tf.reduce_mean(first_inputs, axis=0))) # mean along the batch\n",
    "\n",
    "inputs = first_inputs\n",
    "state = first_state  \n",
    "predictions = np.empty((inference_batch_size,0), dtype = np.int32) \n",
    "\n",
    "print(\"\\n predictions shape:\", predictions.shape)\n",
    "print(\"\\n predictions:\", predictions)\n",
    "\n",
    "for j in range(maximum_iterations):\n",
    "    outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
    "    inputs = next_inputs\n",
    "    state = next_state\n",
    "    outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
    "    predictions = np.append(predictions, outputs, axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discard translations on encountering first sequence \\<end\\> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token = Y_tokenizer.word_index['<end>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Equation:\")\n",
    "print(input_raw)\n",
    "integral_seq = []\n",
    "print(\"\\nIntegral Equation:\")\n",
    "for i in range(len(predictions)):\n",
    "    line = predictions[i,:]\n",
    "    seq = list(itertools.takewhile( lambda index: index !=end_token, line))\n",
    "    output = \" \".join( [Y_tokenizer.index_word[w] for w in seq])\n",
    "    integral_seq.append(output)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infix_prefix1 import prefix_to_infix\n",
    "from sympy import *\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "\n",
    "x = symbols('x', real=True)\n",
    "\n",
    "for ele in integral_seq:\n",
    "    result_infix = prefix_to_infix(ele)\n",
    "    result_infix = result_infix.replace(\"pow\", \"**\")\n",
    "    print(result_infix)\n",
    "    result_simp = simplify(parse_expr(result_infix, local_dict={'x': x}))\n",
    "    print(result_simp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference using Beam Search with beam_width = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 5\n",
    "#use with scope /cpu:0 for inferencing\n",
    "#restore from latest checkpoint for inferencing\n",
    "input_raw=\"1  \\nx\"\n",
    "#input_raw=\"Wow!\"  #checking translation on training set record\n",
    "#def inference(input_raw):\n",
    "input_lines = input_raw.split(\"\\n\")\n",
    "# We have a transcript file containing English-Hindi pairs\n",
    "# Preprocess X\n",
    "input_lines = [preprocess_sentence(line) for line in input_lines]\n",
    "input_sequences = [[X_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
    "                                                                maxlen=Tx, padding='post')\n",
    "inp = tf.convert_to_tensor(input_sequences)\n",
    "#print(inp.shape)\n",
    "inference_batch_size = input_sequences.shape[0]\n",
    "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
    "                              tf.zeros((inference_batch_size, rnn_units))]\n",
    "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
    "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
    "                                                initial_state =encoder_initial_cell_state)\n",
    "\n",
    "start_tokens = tf.fill([inference_batch_size],Y_tokenizer.word_index['<start>'])\n",
    "#print(start_tokens)\n",
    "end_token = Y_tokenizer.word_index['<end>']\n",
    "\n",
    "\n",
    "\n",
    "decoder_input = tf.expand_dims([Y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
    "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "\n",
    "#From official documentation\n",
    "#NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
    "\n",
    "#The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
    "#The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
    "#The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
    "encoder_memory = tfa.seq2seq.tile_batch(a, beam_width)\n",
    "decoderNetwork.attention_mechanism.setup_memory(encoder_memory)\n",
    "print(\"beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] :\", encoder_memory.shape)\n",
    "#set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
    "decoder_initial_state = decoderNetwork.rnn_cell.get_initial_state(batch_size = inference_batch_size* beam_width,dtype = Dtype)\n",
    "encoder_state = tfa.seq2seq.tile_batch([a_tx, c_tx], multiplier=beam_width)\n",
    "decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
    "\n",
    "decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNetwork.rnn_cell,beam_width=beam_width,\n",
    "                                                 output_layer=decoderNetwork.dense_layer)\n",
    "\n",
    "\n",
    "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
    "# One heuristic is to decode up to two times the source sentence lengths.\n",
    "maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
    "\n",
    "#initialize inference decoder\n",
    "\n",
    "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
    "                             start_tokens = start_tokens,\n",
    "                             end_token=end_token,\n",
    "                             initial_state = decoder_initial_state)\n",
    "#print( first_finished.shape)\n",
    "print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
    "\n",
    "inputs = first_inputs\n",
    "state = first_state  \n",
    "predictions = np.empty((inference_batch_size, beam_width,0), dtype = np.int32)\n",
    "beam_scores =  np.empty((inference_batch_size, beam_width,0), dtype = np.float32)                                                                            \n",
    "for j in range(maximum_iterations):\n",
    "    beam_search_outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
    "    inputs = next_inputs\n",
    "    state = next_state\n",
    "    outputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)\n",
    "    scores = np.expand_dims(beam_search_outputs.scores,axis = -1)\n",
    "    predictions = np.append(predictions, outputs, axis = -1)\n",
    "    beam_scores = np.append(beam_scores, scores, axis = -1)\n",
    "print(predictions.shape) \n",
    "print(beam_scores.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------\")\n",
    "print(\"Equation:\")\n",
    "print(input_raw)\n",
    "print(\"-----------------\")\n",
    "print(\"\\nIntegral Equation:\")\n",
    "for i in range(len(predictions)):\n",
    "    print(\"---------------------------------------------\")\n",
    "    output_beams_per_sample = predictions[i,:,:]\n",
    "    score_beams_per_sample = beam_scores[i,:,:]\n",
    "    for beam, score in zip(output_beams_per_sample,score_beams_per_sample) :\n",
    "        seq = list(itertools.takewhile( lambda index: index !=end_token, beam))\n",
    "        score_indexes = np.arange(len(seq))\n",
    "        beam_score = score[score_indexes].sum()\n",
    "        print(\" \".join( [Y_tokenizer.index_word[w] for w in seq]), \" beam score: \", beam_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(input_batch, output_batch,encoder_initial_cell_state, BATCH_SIZE):\n",
    "    #initialize loss = 0\n",
    "    loss = 0\n",
    "\n",
    "    # we can do initialization in outer block\n",
    "    #encoder_initial_cell_state = encoder.initialize_initial_state()\n",
    "    encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
    "    a, h_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
    "                                                    initial_state =encoder_initial_cell_state)\n",
    "\n",
    "\n",
    "\n",
    "    decoder_input = output_batch[:,:-1] # ignore <end>\n",
    "    #compare logits with timestepped +1 version of decoder_input\n",
    "    decoder_output = output_batch[:,1:] #ignore <start>\n",
    "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(decoderNetwork.rnn_cell, \n",
    "                                                greedy_sampler,\n",
    "                                                decoderNetwork.dense_layer)\n",
    "    #BasicDecoderOutput\n",
    "\n",
    "    decoderNetwork.attention_mechanism.setup_memory(a)\n",
    "    #pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
    "    \n",
    "    decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
    "                                                                       encoder_state=[h_tx, c_tx],\n",
    "                                                                       Dtype=tf.float32)\n",
    "    outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
    "                                           sequence_length=BATCH_SIZE*[Ty-1])\n",
    "    logits = outputs.rnn_output\n",
    "    sample_id = outputs.sample_id\n",
    "    #Calculate loss\n",
    "    loss = loss_function(logits, decoder_output)\n",
    "    return loss, sample_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Loss on Entire Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(len(X_test))\n",
    "for (input_batch, output_batch) in dataset_test.take(-1):\n",
    "    batch_size = len(input_batch)\n",
    "    print(input_batch.shape)\n",
    "    encoder_initial_cell_state = [tf.zeros((batch_size, rnn_units)),\n",
    "                                  tf.zeros((batch_size, rnn_units))]\n",
    "    loss,_ = eval_step(input_batch, output_batch, encoder_initial_cell_state, batch_size)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    print(\"Training loss {}\".format(loss) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BasicDecoder initialization returns the <start> sequence as first_input\n",
    "#Check Inference Cell output\n",
    "\n",
    "start_index = Y_tokenizer.word_index['<start>']\n",
    "start_index = tf.constant([start_index], dtype = tf.int32)\n",
    "print(start_index)\n",
    "start_index_emb = decoderNetwork.decoder_embedding(start_index)\n",
    "print(start_index_emb.shape)\n",
    "start_index_emb_avg = tf.reduce_sum(start_index_emb)\n",
    "print(start_index_emb_avg.numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_36_env3",
   "language": "python",
   "name": "py_36_env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
