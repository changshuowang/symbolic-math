{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_qNSzzyaCbD"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "jmjh290raIky"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Transformer model for language understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/transformer\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/transformer.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-f8TnGpE_ex"
   },
   "source": [
    "This tutorial trains a <a href=\"https://arxiv.org/abs/1706.03762\" class=\"external\">Transformer model</a> to translate Portuguese to English. This is an advanced example that assumes knowledge of [text generation](text_generation.ipynb) and [attention](nmt_with_attention.ipynb).\n",
    "\n",
    "The core idea behind the Transformer model is *self-attention*â€”the ability to attend to different positions of the input sequence to compute a representation of that sequence. Transformer creates stacks of self-attention layers and is explained below in the sections *Scaled dot product attention* and *Multi-head attention*.\n",
    "\n",
    "A transformer model handles variable-sized input using stacks of self-attention layers instead of [RNNs](text_classification_rnn.ipynb) or [CNNs](../images/intro_to_cnns.ipynb). This general architecture has a number of advantages:\n",
    "\n",
    "* It make no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects (for example, [StarCraft units](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/#block-8)).\n",
    "* Layer outputs can be calculated in parallel, instead of a series like an RNN.\n",
    "* Distant items can affect each other's output without passing through many RNN-steps, or convolution layers (see [Scene Memory Transformer](https://arxiv.org/pdf/1903.03878.pdf) for example).\n",
    "* It can learn long-range dependencies. This is a challenge in many sequence tasks.\n",
    "\n",
    "The downsides of this architecture are:\n",
    "\n",
    "* For a time-series, the output for a time-step is calculated from the *entire history* instead of only the inputs and current hidden-state. This _may_ be less efficient.   \n",
    "* If the input *does* have a  temporal/spatial relationship, like text, some positional encoding must be added or the model will effectively see a bag of words. \n",
    "\n",
    "After training the model in this notebook, you will be able to input a Portuguese sentence and return the English translation.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png\" width=\"800\" alt=\"Attention heatmap\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JjJJyJTZYebt"
   },
   "outputs": [],
   "source": [
    "# !pip install -q tf-nightly\n",
    "# import tensorflow_datasets as tfds\n",
    "# import tensorflow as tf\n",
    "\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fd1NWMxjfsDd"
   },
   "source": [
    "## Setup input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4_Qt8W1hJE_"
   },
   "source": [
    "Use [TFDS](https://www.tensorflow.org/datasets) to load the [Portugese-English translation dataset](https://github.com/neulab/word-embeddings-for-nmt) from the [TED Talks Open Translation Project](https://www.ted.com/participate/translate).\n",
    "\n",
    "This dataset contains approximately 50000 training examples, 1100 validation examples, and 2000 test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8q9t4FmN96eN"
   },
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCEKotqosGfq"
   },
   "source": [
    "Create a custom subwords tokenizer from the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVBg5Q8tBk5z"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DYWukNFkGQN"
   },
   "outputs": [],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9KJWJjrsZ4Y"
   },
   "source": [
    "The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bf2ntBxjkqK6"
   },
   "outputs": [],
   "source": [
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcRp7VcQ5m6g"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGi4PoVakxdc"
   },
   "source": [
    "Add a start and end token to the input and target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZwnPr4R055s"
   },
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "  return lang1, lang2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tx1sFbR-9fRs"
   },
   "source": [
    "You want to use `Dataset.map` to apply this function to each element of the dataset.  `Dataset.map` runs in graph mode.\n",
    "\n",
    "* Graph tensors do not have a value. \n",
    "* In graph mode you can only use TensorFlow Ops and functions. \n",
    "\n",
    "So you can't `.map` this function directly: You need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mah1cS-P70Iz"
   },
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "  result_pt.set_shape([None])\n",
    "  result_en.set_shape([None])\n",
    "\n",
    "  return result_pt, result_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6JrGp5Gek6Ql"
   },
   "source": [
    "Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QEgbjntk6Yf"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c081xPGv1CPI"
   },
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "  return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7SiL41mbiCZ"
   },
   "outputs": [],
   "source": [
    "train_preprocessed = (\n",
    "    train_examples\n",
    "    .map(tf_encode) \n",
    "    .filter(filter_max_length)\n",
    "    # cache the dataset to memory to get a speedup while reading from it.\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE))\n",
    "\n",
    "val_preprocessed = (\n",
    "    val_examples\n",
    "    .map(tf_encode)\n",
    "    .filter(filter_max_length))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7oo6Z4xUcMmt"
   },
   "source": [
    "Pad and batch examples together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mk9AZdZ5bcS"
   },
   "outputs": [],
   "source": [
    "train_dataset = (train_preprocessed\n",
    "                 .padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "\n",
    "val_dataset = (val_preprocessed\n",
    "               .padded_batch(BATCH_SIZE,  padded_shapes=([None], [None])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpLOt3T9WtKo"
   },
   "source": [
    "Note: As of **TensorFlow 2.2** the padded_shapes argument is no longer required. The default behavior is to pad all axes to the longest in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByTGAOY3XSyo"
   },
   "outputs": [],
   "source": [
    "train_dataset = (train_preprocessed\n",
    "                 .padded_batch(BATCH_SIZE)\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "\n",
    "val_dataset = (val_preprocessed\n",
    "               .padded_batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2r32dOx4XVHP"
   },
   "source": [
    "Get an batch from the validation set to test the code on later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fXvfYVfQr2n"
   },
   "outputs": [],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "print(tf.__version__)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "#import io\n",
    "import numpy as np\n",
    "# import re\n",
    "import unicodedata\n",
    "# import urllib3\n",
    "# import shutil\n",
    "# import zipfile\n",
    "# import itertools\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/cleaned_data/equation_train1.txt', 'r')\n",
    "equation = f.read().splitlines()[:num_samples]\n",
    "f = open('./data/cleaned_data/integration_train1.txt', 'r')\n",
    "integration = f.read().splitlines()[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(inp, sequence_length):\n",
    "    \"\"\" word to index \"\"\"\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(inp)\n",
    "    sequences = tokenizer.texts_to_sequences(inp)\n",
    "    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post', maxlen=sequence_length, truncating='post')\n",
    "    return  sequences, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 64\n",
    "# Tokenize each word into index and return the tokenized list and tokenizer\n",
    "X , X_tokenizer = tokenize(equation, sequence_length)\n",
    "Y,  Y_tokenizer = tokenize(integration, sequence_length)\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size :  34\n",
      "output_vocab_size :  34\n"
     ]
    }
   ],
   "source": [
    "input_vocab_size = len(X_tokenizer.word_index) + 1 \n",
    "output_vocab_size = len(Y_tokenizer.word_index)+ 1\n",
    "\n",
    "print(\"input_vocab_size : \", input_vocab_size)\n",
    "print(\"output_vocab_size : \" ,output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBQuibYA4n0n"
   },
   "source": [
    "## Positional encoding\n",
    "\n",
    "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
    "\n",
    "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
    "\n",
    "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WhIOZjMNKujn"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rz82wEs5biZ"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kLCla68EloE"
   },
   "outputs": [],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_b4ou4TYqUN"
   },
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s42Uydjkv0hF"
   },
   "source": [
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2i8-e1s8ti9"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7BYeBCNvi7n"
   },
   "outputs": [],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0hzukDBgVom"
   },
   "source": [
    "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
    "\n",
    "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVxS8OPI9uI0"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxKGuXxaBeeE"
   },
   "outputs": [],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xluDl5cXYy4y"
   },
   "source": [
    "## Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsxEE_-Wa1gF"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
    "\n",
    "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. \n",
    "\n",
    "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, and you get a gentler softmax.\n",
    "\n",
    "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LazzUq3bJ5SH"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiqETnhCkoXh"
   },
   "source": [
    "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
    "\n",
    "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n90YjClyInFy"
   },
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAzUAf2DPlNt"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zg6k-fGhgXra"
   },
   "outputs": [],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAq3YOzUgXhb"
   },
   "outputs": [],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aOz-4_XIhaTP"
   },
   "source": [
    "Pass all the queries together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dlU8Tm-hYrF"
   },
   "outputs": [],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmzGPEy64qmA"
   },
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fz5BMC8Kaoqo"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "\n",
    "Multi-head attention consists of four parts:\n",
    "*    Linear layers and split into heads.\n",
    "*    Scaled dot-product attention.\n",
    "*    Concatenation of heads.\n",
    "*    Final linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPmbr6F1C-v_"
   },
   "source": [
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
    "\n",
    "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
    "\n",
    "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSV3PPKsYecw"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D8FJue5lDyZ"
   },
   "source": [
    "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hu94p-_-2_BX"
   },
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdDqGayx67vv"
   },
   "source": [
    "## Point wise feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBqzJXGfHK3X"
   },
   "source": [
    "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ET7xLt0yCT6Z"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mytb1lPyOHLB"
   },
   "outputs": [],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "## Encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yScbC0MUH8dS"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfYJG-Kvgwy2"
   },
   "source": [
    "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
    "\n",
    "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
    "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFv-FNYUmvpn"
   },
   "source": [
    "### Encoder layer\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1.   Multi-head attention (with padding mask) \n",
    "2.    Point wise feed forward networks. \n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncyS-Ms3i2x_"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzZRXdO0mI48"
   },
   "outputs": [],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LO_48Owmx_o"
   },
   "source": [
    "### Decoder layer\n",
    "\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
    "3.   Point wise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
    "\n",
    "There are N decoder layers in the transformer.\n",
    "\n",
    "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SoX0-vd1hue"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ne2Bqx8k71l0"
   },
   "outputs": [],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SE1H51Ajm0q1"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "The `Encoder` consists of:\n",
    "1.   Input Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N encoder layers\n",
    "\n",
    "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpEox7gJ8FCI"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QG9nueFQKXx"
   },
   "outputs": [],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maximum_position_encoding=10000)\n",
    "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-uO6ls8m2O5"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtT7PKzrXkNr"
   },
   "source": [
    " The `Decoder` consists of:\n",
    "1.   Output Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5_d5-PLQXwY"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1jXoAMRZyvu"
   },
   "outputs": [],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_position_encoding=5000)\n",
    "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False,\n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y54xnJnuYgJ7"
   },
   "source": [
    "## Create the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uERO1y54cOKq"
   },
   "source": [
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PED3bIpOYkBu"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJ4fbQcIkHW1"
   },
   "outputs": [],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsINyf1VEQLC"
   },
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVjWCxFNcgbt"
   },
   "source": [
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
    "\n",
    "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
    "\n",
    "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnJn5SLA2ahP"
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 4\n",
    "\n",
    "# input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "# target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYEGhEOtzn5W"
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOmWW--yP3zx"
   },
   "source": [
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYQdOO1axwEI"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7r4scdulztRx"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f33ZCgvHpPdG"
   },
   "outputs": [],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgkDE7hzo8r5"
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxGJtoDuYIHL"
   },
   "source": [
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlhsJMm0TW_B"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67oqVHiT0Eiu"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phlyxMnm-Tpx"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeHumfr7zmMa"
   },
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiysUa--4tOU"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, output_vocab_size, \n",
    "                          pe_input=sequence_length, \n",
    "                          pe_target=sequence_length,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOJUSB1T8GjM"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fzuf06YZp66w"
   },
   "source": [
    "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNhuYfllndLZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Di_Yaa1gf9r"
   },
   "source": [
    "The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
    "\n",
    "For example, `sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n",
    "\n",
    "`tar_real` = \"A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
    "\n",
    "During training this example uses teacher-forcing (like in the [text generation tutorial](./text_generation.ipynb)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
    "\n",
    "As the transformer predicts each word, *self-attention* allows it to look at the previous words in the input sequence to better predict the next word.\n",
    "\n",
    "To prevent the model from peaking at the expected output the model uses a look-ahead mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKpoA6q1sJFj"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJwmp9OE29oj"
   },
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(transformer.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=test_step_signature)\n",
    "def test_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    predictions = transformer(inp, tar_inp, \n",
    "                                 False, \n",
    "                                  enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "\n",
    "    test_loss(loss)\n",
    "    test_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qM2PDWGDJ_8V"
   },
   "source": [
    "Portuguese is used as the input language and English is the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbvmaKNiznHZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.007231473922729492 secs\n",
      "\n",
      "Epoch 2 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048716068267822266 secs\n",
      "\n",
      "Epoch 3 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004322052001953125 secs\n",
      "\n",
      "Epoch 4 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004544734954833984 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1699\n",
      "Epoch 5 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.021699190139770508 secs\n",
      "\n",
      "Epoch 6 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005071401596069336 secs\n",
      "\n",
      "Epoch 7 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0051326751708984375 secs\n",
      "\n",
      "Epoch 8 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004908323287963867 secs\n",
      "\n",
      "Epoch 9 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005300998687744141 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-1700\n",
      "Epoch 10 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.018751144409179688 secs\n",
      "\n",
      "Epoch 11 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047414302825927734 secs\n",
      "\n",
      "Epoch 12 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004650592803955078 secs\n",
      "\n",
      "Epoch 13 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004405021667480469 secs\n",
      "\n",
      "Epoch 14 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004555940628051758 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-1701\n",
      "Epoch 15 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016833066940307617 secs\n",
      "\n",
      "Epoch 16 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005156517028808594 secs\n",
      "\n",
      "Epoch 17 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00475311279296875 secs\n",
      "\n",
      "Epoch 18 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004667043685913086 secs\n",
      "\n",
      "Epoch 19 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004239797592163086 secs\n",
      "\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-1702\n",
      "Epoch 20 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01660919189453125 secs\n",
      "\n",
      "Epoch 21 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004607677459716797 secs\n",
      "\n",
      "Epoch 22 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004900217056274414 secs\n",
      "\n",
      "Epoch 23 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005438566207885742 secs\n",
      "\n",
      "Epoch 24 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004743814468383789 secs\n",
      "\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-1703\n",
      "Epoch 25 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01712965965270996 secs\n",
      "\n",
      "Epoch 26 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004848003387451172 secs\n",
      "\n",
      "Epoch 27 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004428386688232422 secs\n",
      "\n",
      "Epoch 28 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004507780075073242 secs\n",
      "\n",
      "Epoch 29 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0049343109130859375 secs\n",
      "\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-1704\n",
      "Epoch 30 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015849590301513672 secs\n",
      "\n",
      "Epoch 31 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00456690788269043 secs\n",
      "\n",
      "Epoch 32 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004678487777709961 secs\n",
      "\n",
      "Epoch 33 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046694278717041016 secs\n",
      "\n",
      "Epoch 34 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004580259323120117 secs\n",
      "\n",
      "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-1705\n",
      "Epoch 35 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015205144882202148 secs\n",
      "\n",
      "Epoch 36 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00462031364440918 secs\n",
      "\n",
      "Epoch 37 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004602909088134766 secs\n",
      "\n",
      "Epoch 38 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004658699035644531 secs\n",
      "\n",
      "Epoch 39 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004370212554931641 secs\n",
      "\n",
      "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-1706\n",
      "Epoch 40 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014963626861572266 secs\n",
      "\n",
      "Epoch 41 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004506826400756836 secs\n",
      "\n",
      "Epoch 42 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00462031364440918 secs\n",
      "\n",
      "Epoch 43 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046498775482177734 secs\n",
      "\n",
      "Epoch 44 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00463414192199707 secs\n",
      "\n",
      "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-1707\n",
      "Epoch 45 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015112638473510742 secs\n",
      "\n",
      "Epoch 46 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004604339599609375 secs\n",
      "\n",
      "Epoch 47 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004502773284912109 secs\n",
      "\n",
      "Epoch 48 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00464177131652832 secs\n",
      "\n",
      "Epoch 49 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004750967025756836 secs\n",
      "\n",
      "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-1708\n",
      "Epoch 50 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01511836051940918 secs\n",
      "\n",
      "Epoch 51 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047779083251953125 secs\n",
      "\n",
      "Epoch 52 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048749446868896484 secs\n",
      "\n",
      "Epoch 53 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004693269729614258 secs\n",
      "\n",
      "Epoch 54 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004684925079345703 secs\n",
      "\n",
      "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-1709\n",
      "Epoch 55 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014948844909667969 secs\n",
      "\n",
      "Epoch 56 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004651308059692383 secs\n",
      "\n",
      "Epoch 57 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004574298858642578 secs\n",
      "\n",
      "Epoch 58 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0049703121185302734 secs\n",
      "\n",
      "Epoch 59 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005268096923828125 secs\n",
      "\n",
      "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-1710\n",
      "Epoch 60 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01536107063293457 secs\n",
      "\n",
      "Epoch 61 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004514932632446289 secs\n",
      "\n",
      "Epoch 62 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004418849945068359 secs\n",
      "\n",
      "Epoch 63 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004481315612792969 secs\n",
      "\n",
      "Epoch 64 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004611968994140625 secs\n",
      "\n",
      "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-1711\n",
      "Epoch 65 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01506662368774414 secs\n",
      "\n",
      "Epoch 66 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004541635513305664 secs\n",
      "\n",
      "Epoch 67 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00446319580078125 secs\n",
      "\n",
      "Epoch 68 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004209041595458984 secs\n",
      "\n",
      "Epoch 69 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004545688629150391 secs\n",
      "\n",
      "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-1712\n",
      "Epoch 70 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015748262405395508 secs\n",
      "\n",
      "Epoch 71 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005219936370849609 secs\n",
      "\n",
      "Epoch 72 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005020856857299805 secs\n",
      "\n",
      "Epoch 73 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0049648284912109375 secs\n",
      "\n",
      "Epoch 74 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004636526107788086 secs\n",
      "\n",
      "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-1713\n",
      "Epoch 75 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015845775604248047 secs\n",
      "\n",
      "Epoch 76 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004599094390869141 secs\n",
      "\n",
      "Epoch 77 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004671573638916016 secs\n",
      "\n",
      "Epoch 78 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004425764083862305 secs\n",
      "\n",
      "Epoch 79 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004510402679443359 secs\n",
      "\n",
      "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-1714\n",
      "Epoch 80 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016115665435791016 secs\n",
      "\n",
      "Epoch 81 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004799365997314453 secs\n",
      "\n",
      "Epoch 82 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004396200180053711 secs\n",
      "\n",
      "Epoch 83 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004571199417114258 secs\n",
      "\n",
      "Epoch 84 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004355192184448242 secs\n",
      "\n",
      "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-1715\n",
      "Epoch 85 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01502227783203125 secs\n",
      "\n",
      "Epoch 86 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004492282867431641 secs\n",
      "\n",
      "Epoch 87 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004605293273925781 secs\n",
      "\n",
      "Epoch 88 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047168731689453125 secs\n",
      "\n",
      "Epoch 89 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004594087600708008 secs\n",
      "\n",
      "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-1716\n",
      "Epoch 90 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015143871307373047 secs\n",
      "\n",
      "Epoch 91 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004698276519775391 secs\n",
      "\n",
      "Epoch 92 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004722118377685547 secs\n",
      "\n",
      "Epoch 93 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004549264907836914 secs\n",
      "\n",
      "Epoch 94 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004469633102416992 secs\n",
      "\n",
      "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-1717\n",
      "Epoch 95 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015465259552001953 secs\n",
      "\n",
      "Epoch 96 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004674434661865234 secs\n",
      "\n",
      "Epoch 97 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004767656326293945 secs\n",
      "\n",
      "Epoch 98 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004547119140625 secs\n",
      "\n",
      "Epoch 99 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044269561767578125 secs\n",
      "\n",
      "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-1718\n",
      "Epoch 100 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015395402908325195 secs\n",
      "\n",
      "Epoch 101 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045282840728759766 secs\n",
      "\n",
      "Epoch 102 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045316219329833984 secs\n",
      "\n",
      "Epoch 103 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047304630279541016 secs\n",
      "\n",
      "Epoch 104 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045318603515625 secs\n",
      "\n",
      "Saving checkpoint for epoch 105 at ./checkpoints/train/ckpt-1719\n",
      "Epoch 105 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015355825424194336 secs\n",
      "\n",
      "Epoch 106 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00464177131652832 secs\n",
      "\n",
      "Epoch 107 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004881381988525391 secs\n",
      "\n",
      "Epoch 108 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004500389099121094 secs\n",
      "\n",
      "Epoch 109 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004540443420410156 secs\n",
      "\n",
      "Saving checkpoint for epoch 110 at ./checkpoints/train/ckpt-1720\n",
      "Epoch 110 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015298128128051758 secs\n",
      "\n",
      "Epoch 111 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004505157470703125 secs\n",
      "\n",
      "Epoch 112 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004689216613769531 secs\n",
      "\n",
      "Epoch 113 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004661083221435547 secs\n",
      "\n",
      "Epoch 114 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004502773284912109 secs\n",
      "\n",
      "Saving checkpoint for epoch 115 at ./checkpoints/train/ckpt-1721\n",
      "Epoch 115 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015485286712646484 secs\n",
      "\n",
      "Epoch 116 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045735836029052734 secs\n",
      "\n",
      "Epoch 117 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00460505485534668 secs\n",
      "\n",
      "Epoch 118 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004571437835693359 secs\n",
      "\n",
      "Epoch 119 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0050427913665771484 secs\n",
      "\n",
      "Saving checkpoint for epoch 120 at ./checkpoints/train/ckpt-1722\n",
      "Epoch 120 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015761613845825195 secs\n",
      "\n",
      "Epoch 121 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004760026931762695 secs\n",
      "\n",
      "Epoch 122 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004526853561401367 secs\n",
      "\n",
      "Epoch 123 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048024654388427734 secs\n",
      "\n",
      "Epoch 124 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045871734619140625 secs\n",
      "\n",
      "Saving checkpoint for epoch 125 at ./checkpoints/train/ckpt-1723\n",
      "Epoch 125 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0157318115234375 secs\n",
      "\n",
      "Epoch 126 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004652261734008789 secs\n",
      "\n",
      "Epoch 127 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004575252532958984 secs\n",
      "\n",
      "Epoch 128 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004647970199584961 secs\n",
      "\n",
      "Epoch 129 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004616737365722656 secs\n",
      "\n",
      "Saving checkpoint for epoch 130 at ./checkpoints/train/ckpt-1724\n",
      "Epoch 130 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015537738800048828 secs\n",
      "\n",
      "Epoch 131 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004691362380981445 secs\n",
      "\n",
      "Epoch 132 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004706382751464844 secs\n",
      "\n",
      "Epoch 133 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004723310470581055 secs\n",
      "\n",
      "Epoch 134 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004904985427856445 secs\n",
      "\n",
      "Saving checkpoint for epoch 135 at ./checkpoints/train/ckpt-1725\n",
      "Epoch 135 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01604008674621582 secs\n",
      "\n",
      "Epoch 136 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004765748977661133 secs\n",
      "\n",
      "Epoch 137 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00461578369140625 secs\n",
      "\n",
      "Epoch 138 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004686117172241211 secs\n",
      "\n",
      "Epoch 139 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00464320182800293 secs\n",
      "\n",
      "Saving checkpoint for epoch 140 at ./checkpoints/train/ckpt-1726\n",
      "Epoch 140 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015274286270141602 secs\n",
      "\n",
      "Epoch 141 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004624128341674805 secs\n",
      "\n",
      "Epoch 142 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004673004150390625 secs\n",
      "\n",
      "Epoch 143 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004712343215942383 secs\n",
      "\n",
      "Epoch 144 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047910213470458984 secs\n",
      "\n",
      "Saving checkpoint for epoch 145 at ./checkpoints/train/ckpt-1727\n",
      "Epoch 145 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016215085983276367 secs\n",
      "\n",
      "Epoch 146 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004597187042236328 secs\n",
      "\n",
      "Epoch 147 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004704475402832031 secs\n",
      "\n",
      "Epoch 148 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004549980163574219 secs\n",
      "\n",
      "Epoch 149 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004450082778930664 secs\n",
      "\n",
      "Saving checkpoint for epoch 150 at ./checkpoints/train/ckpt-1728\n",
      "Epoch 150 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014906644821166992 secs\n",
      "\n",
      "Epoch 151 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00458836555480957 secs\n",
      "\n",
      "Epoch 152 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004483699798583984 secs\n",
      "\n",
      "Epoch 153 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004605293273925781 secs\n",
      "\n",
      "Epoch 154 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004549741744995117 secs\n",
      "\n",
      "Saving checkpoint for epoch 155 at ./checkpoints/train/ckpt-1729\n",
      "Epoch 155 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015470743179321289 secs\n",
      "\n",
      "Epoch 156 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045375823974609375 secs\n",
      "\n",
      "Epoch 157 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004514217376708984 secs\n",
      "\n",
      "Epoch 158 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004691123962402344 secs\n",
      "\n",
      "Epoch 159 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046503543853759766 secs\n",
      "\n",
      "Saving checkpoint for epoch 160 at ./checkpoints/train/ckpt-1730\n",
      "Epoch 160 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015811443328857422 secs\n",
      "\n",
      "Epoch 161 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00476837158203125 secs\n",
      "\n",
      "Epoch 162 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005122184753417969 secs\n",
      "\n",
      "Epoch 163 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044519901275634766 secs\n",
      "\n",
      "Epoch 164 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004659414291381836 secs\n",
      "\n",
      "Saving checkpoint for epoch 165 at ./checkpoints/train/ckpt-1731\n",
      "Epoch 165 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015320777893066406 secs\n",
      "\n",
      "Epoch 166 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004594564437866211 secs\n",
      "\n",
      "Epoch 167 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004487037658691406 secs\n",
      "\n",
      "Epoch 168 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004667758941650391 secs\n",
      "\n",
      "Epoch 169 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004643440246582031 secs\n",
      "\n",
      "Saving checkpoint for epoch 170 at ./checkpoints/train/ckpt-1732\n",
      "Epoch 170 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015258312225341797 secs\n",
      "\n",
      "Epoch 171 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004837989807128906 secs\n",
      "\n",
      "Epoch 172 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00461125373840332 secs\n",
      "\n",
      "Epoch 173 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004617214202880859 secs\n",
      "\n",
      "Epoch 174 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004511117935180664 secs\n",
      "\n",
      "Saving checkpoint for epoch 175 at ./checkpoints/train/ckpt-1733\n",
      "Epoch 175 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016001462936401367 secs\n",
      "\n",
      "Epoch 176 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004925727844238281 secs\n",
      "\n",
      "Epoch 177 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005772113800048828 secs\n",
      "\n",
      "Epoch 178 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004674434661865234 secs\n",
      "\n",
      "Epoch 179 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004659891128540039 secs\n",
      "\n",
      "Saving checkpoint for epoch 180 at ./checkpoints/train/ckpt-1734\n",
      "Epoch 180 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016020774841308594 secs\n",
      "\n",
      "Epoch 181 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004664182662963867 secs\n",
      "\n",
      "Epoch 182 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004651069641113281 secs\n",
      "\n",
      "Epoch 183 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004570960998535156 secs\n",
      "\n",
      "Epoch 184 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00451970100402832 secs\n",
      "\n",
      "Saving checkpoint for epoch 185 at ./checkpoints/train/ckpt-1735\n",
      "Epoch 185 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015708446502685547 secs\n",
      "\n",
      "Epoch 186 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004760265350341797 secs\n",
      "\n",
      "Epoch 187 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045239925384521484 secs\n",
      "\n",
      "Epoch 188 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004521369934082031 secs\n",
      "\n",
      "Epoch 189 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004614353179931641 secs\n",
      "\n",
      "Saving checkpoint for epoch 190 at ./checkpoints/train/ckpt-1736\n",
      "Epoch 190 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01560521125793457 secs\n",
      "\n",
      "Epoch 191 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00445246696472168 secs\n",
      "\n",
      "Epoch 192 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00482630729675293 secs\n",
      "\n",
      "Epoch 193 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004420042037963867 secs\n",
      "\n",
      "Epoch 194 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004572868347167969 secs\n",
      "\n",
      "Saving checkpoint for epoch 195 at ./checkpoints/train/ckpt-1737\n",
      "Epoch 195 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01477956771850586 secs\n",
      "\n",
      "Epoch 196 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004656314849853516 secs\n",
      "\n",
      "Epoch 197 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004325151443481445 secs\n",
      "\n",
      "Epoch 198 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048024654388427734 secs\n",
      "\n",
      "Epoch 199 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004630327224731445 secs\n",
      "\n",
      "Saving checkpoint for epoch 200 at ./checkpoints/train/ckpt-1738\n",
      "Epoch 200 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015362977981567383 secs\n",
      "\n",
      "Epoch 201 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004517316818237305 secs\n",
      "\n",
      "Epoch 202 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004620790481567383 secs\n",
      "\n",
      "Epoch 203 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004877567291259766 secs\n",
      "\n",
      "Epoch 204 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047092437744140625 secs\n",
      "\n",
      "Saving checkpoint for epoch 205 at ./checkpoints/train/ckpt-1739\n",
      "Epoch 205 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016048431396484375 secs\n",
      "\n",
      "Epoch 206 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004678010940551758 secs\n",
      "\n",
      "Epoch 207 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005141019821166992 secs\n",
      "\n",
      "Epoch 208 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004537343978881836 secs\n",
      "\n",
      "Epoch 209 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004877328872680664 secs\n",
      "\n",
      "Saving checkpoint for epoch 210 at ./checkpoints/train/ckpt-1740\n",
      "Epoch 210 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015406608581542969 secs\n",
      "\n",
      "Epoch 211 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005152463912963867 secs\n",
      "\n",
      "Epoch 212 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005129337310791016 secs\n",
      "\n",
      "Epoch 213 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004618406295776367 secs\n",
      "\n",
      "Epoch 214 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004589080810546875 secs\n",
      "\n",
      "Saving checkpoint for epoch 215 at ./checkpoints/train/ckpt-1741\n",
      "Epoch 215 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015503406524658203 secs\n",
      "\n",
      "Epoch 216 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004652261734008789 secs\n",
      "\n",
      "Epoch 217 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004683256149291992 secs\n",
      "\n",
      "Epoch 218 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004702091217041016 secs\n",
      "\n",
      "Epoch 219 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004889726638793945 secs\n",
      "\n",
      "Saving checkpoint for epoch 220 at ./checkpoints/train/ckpt-1742\n",
      "Epoch 220 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015417337417602539 secs\n",
      "\n",
      "Epoch 221 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044708251953125 secs\n",
      "\n",
      "Epoch 222 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046117305755615234 secs\n",
      "\n",
      "Epoch 223 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004752397537231445 secs\n",
      "\n",
      "Epoch 224 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045931339263916016 secs\n",
      "\n",
      "Saving checkpoint for epoch 225 at ./checkpoints/train/ckpt-1743\n",
      "Epoch 225 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01548147201538086 secs\n",
      "\n",
      "Epoch 226 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004512786865234375 secs\n",
      "\n",
      "Epoch 227 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004478931427001953 secs\n",
      "\n",
      "Epoch 228 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044858455657958984 secs\n",
      "\n",
      "Epoch 229 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004370689392089844 secs\n",
      "\n",
      "Saving checkpoint for epoch 230 at ./checkpoints/train/ckpt-1744\n",
      "Epoch 230 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01464390754699707 secs\n",
      "\n",
      "Epoch 231 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004520893096923828 secs\n",
      "\n",
      "Epoch 232 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0043048858642578125 secs\n",
      "\n",
      "Epoch 233 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004568338394165039 secs\n",
      "\n",
      "Epoch 234 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004754066467285156 secs\n",
      "\n",
      "Saving checkpoint for epoch 235 at ./checkpoints/train/ckpt-1745\n",
      "Epoch 235 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016442298889160156 secs\n",
      "\n",
      "Epoch 236 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004930257797241211 secs\n",
      "\n",
      "Epoch 237 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00470423698425293 secs\n",
      "\n",
      "Epoch 238 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004651308059692383 secs\n",
      "\n",
      "Epoch 239 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004519462585449219 secs\n",
      "\n",
      "Saving checkpoint for epoch 240 at ./checkpoints/train/ckpt-1746\n",
      "Epoch 240 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015227317810058594 secs\n",
      "\n",
      "Epoch 241 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047495365142822266 secs\n",
      "\n",
      "Epoch 242 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046234130859375 secs\n",
      "\n",
      "Epoch 243 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004748344421386719 secs\n",
      "\n",
      "Epoch 244 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046465396881103516 secs\n",
      "\n",
      "Saving checkpoint for epoch 245 at ./checkpoints/train/ckpt-1747\n",
      "Epoch 245 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016389131546020508 secs\n",
      "\n",
      "Epoch 246 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004689931869506836 secs\n",
      "\n",
      "Epoch 247 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004547119140625 secs\n",
      "\n",
      "Epoch 248 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004639387130737305 secs\n",
      "\n",
      "Epoch 249 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004502058029174805 secs\n",
      "\n",
      "Saving checkpoint for epoch 250 at ./checkpoints/train/ckpt-1748\n",
      "Epoch 250 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016284465789794922 secs\n",
      "\n",
      "Epoch 251 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00455927848815918 secs\n",
      "\n",
      "Epoch 252 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004631757736206055 secs\n",
      "\n",
      "Epoch 253 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004706859588623047 secs\n",
      "\n",
      "Epoch 254 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004537343978881836 secs\n",
      "\n",
      "Saving checkpoint for epoch 255 at ./checkpoints/train/ckpt-1749\n",
      "Epoch 255 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01578998565673828 secs\n",
      "\n",
      "Epoch 256 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00473332405090332 secs\n",
      "\n",
      "Epoch 257 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045413970947265625 secs\n",
      "\n",
      "Epoch 258 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004762887954711914 secs\n",
      "\n",
      "Epoch 259 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004559516906738281 secs\n",
      "\n",
      "Saving checkpoint for epoch 260 at ./checkpoints/train/ckpt-1750\n",
      "Epoch 260 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015285015106201172 secs\n",
      "\n",
      "Epoch 261 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044705867767333984 secs\n",
      "\n",
      "Epoch 262 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046656131744384766 secs\n",
      "\n",
      "Epoch 263 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004469394683837891 secs\n",
      "\n",
      "Epoch 264 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046999454498291016 secs\n",
      "\n",
      "Saving checkpoint for epoch 265 at ./checkpoints/train/ckpt-1751\n",
      "Epoch 265 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015683412551879883 secs\n",
      "\n",
      "Epoch 266 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004769563674926758 secs\n",
      "\n",
      "Epoch 267 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00448918342590332 secs\n",
      "\n",
      "Epoch 268 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004605293273925781 secs\n",
      "\n",
      "Epoch 269 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004509449005126953 secs\n",
      "\n",
      "Saving checkpoint for epoch 270 at ./checkpoints/train/ckpt-1752\n",
      "Epoch 270 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015275955200195312 secs\n",
      "\n",
      "Epoch 271 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004792928695678711 secs\n",
      "\n",
      "Epoch 272 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00468134880065918 secs\n",
      "\n",
      "Epoch 273 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048520565032958984 secs\n",
      "\n",
      "Epoch 274 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00458836555480957 secs\n",
      "\n",
      "Saving checkpoint for epoch 275 at ./checkpoints/train/ckpt-1753\n",
      "Epoch 275 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01580023765563965 secs\n",
      "\n",
      "Epoch 276 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046384334564208984 secs\n",
      "\n",
      "Epoch 277 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004842042922973633 secs\n",
      "\n",
      "Epoch 278 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004736423492431641 secs\n",
      "\n",
      "Epoch 279 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004533529281616211 secs\n",
      "\n",
      "Saving checkpoint for epoch 280 at ./checkpoints/train/ckpt-1754\n",
      "Epoch 280 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0159454345703125 secs\n",
      "\n",
      "Epoch 281 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047185420989990234 secs\n",
      "\n",
      "Epoch 282 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004985332489013672 secs\n",
      "\n",
      "Epoch 283 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046770572662353516 secs\n",
      "\n",
      "Epoch 284 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004673004150390625 secs\n",
      "\n",
      "Saving checkpoint for epoch 285 at ./checkpoints/train/ckpt-1755\n",
      "Epoch 285 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015869855880737305 secs\n",
      "\n",
      "Epoch 286 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004710197448730469 secs\n",
      "\n",
      "Epoch 287 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045375823974609375 secs\n",
      "\n",
      "Epoch 288 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004830360412597656 secs\n",
      "\n",
      "Epoch 289 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004689455032348633 secs\n",
      "\n",
      "Saving checkpoint for epoch 290 at ./checkpoints/train/ckpt-1756\n",
      "Epoch 290 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01589035987854004 secs\n",
      "\n",
      "Epoch 291 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004860877990722656 secs\n",
      "\n",
      "Epoch 292 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004556417465209961 secs\n",
      "\n",
      "Epoch 293 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004700422286987305 secs\n",
      "\n",
      "Epoch 294 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004616260528564453 secs\n",
      "\n",
      "Saving checkpoint for epoch 295 at ./checkpoints/train/ckpt-1757\n",
      "Epoch 295 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0161898136138916 secs\n",
      "\n",
      "Epoch 296 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004662275314331055 secs\n",
      "\n",
      "Epoch 297 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004575014114379883 secs\n",
      "\n",
      "Epoch 298 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004984855651855469 secs\n",
      "\n",
      "Epoch 299 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004528045654296875 secs\n",
      "\n",
      "Saving checkpoint for epoch 300 at ./checkpoints/train/ckpt-1758\n",
      "Epoch 300 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01611614227294922 secs\n",
      "\n",
      "Epoch 301 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004572868347167969 secs\n",
      "\n",
      "Epoch 302 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004423856735229492 secs\n",
      "\n",
      "Epoch 303 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004759073257446289 secs\n",
      "\n",
      "Epoch 304 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046918392181396484 secs\n",
      "\n",
      "Saving checkpoint for epoch 305 at ./checkpoints/train/ckpt-1759\n",
      "Epoch 305 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01613593101501465 secs\n",
      "\n",
      "Epoch 306 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004824399948120117 secs\n",
      "\n",
      "Epoch 307 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004797458648681641 secs\n",
      "\n",
      "Epoch 308 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004595041275024414 secs\n",
      "\n",
      "Epoch 309 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004489421844482422 secs\n",
      "\n",
      "Saving checkpoint for epoch 310 at ./checkpoints/train/ckpt-1760\n",
      "Epoch 310 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015722990036010742 secs\n",
      "\n",
      "Epoch 311 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045964717864990234 secs\n",
      "\n",
      "Epoch 312 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004569530487060547 secs\n",
      "\n",
      "Epoch 313 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004580974578857422 secs\n",
      "\n",
      "Epoch 314 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004377603530883789 secs\n",
      "\n",
      "Saving checkpoint for epoch 315 at ./checkpoints/train/ckpt-1761\n",
      "Epoch 315 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01505589485168457 secs\n",
      "\n",
      "Epoch 316 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044422149658203125 secs\n",
      "\n",
      "Epoch 317 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048601627349853516 secs\n",
      "\n",
      "Epoch 318 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004590272903442383 secs\n",
      "\n",
      "Epoch 319 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004728555679321289 secs\n",
      "\n",
      "Saving checkpoint for epoch 320 at ./checkpoints/train/ckpt-1762\n",
      "Epoch 320 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015441656112670898 secs\n",
      "\n",
      "Epoch 321 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00469660758972168 secs\n",
      "\n",
      "Epoch 322 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004624605178833008 secs\n",
      "\n",
      "Epoch 323 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004562854766845703 secs\n",
      "\n",
      "Epoch 324 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004528045654296875 secs\n",
      "\n",
      "Saving checkpoint for epoch 325 at ./checkpoints/train/ckpt-1763\n",
      "Epoch 325 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015858888626098633 secs\n",
      "\n",
      "Epoch 326 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00483393669128418 secs\n",
      "\n",
      "Epoch 327 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004678487777709961 secs\n",
      "\n",
      "Epoch 328 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045969486236572266 secs\n",
      "\n",
      "Epoch 329 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004744529724121094 secs\n",
      "\n",
      "Saving checkpoint for epoch 330 at ./checkpoints/train/ckpt-1764\n",
      "Epoch 330 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015979766845703125 secs\n",
      "\n",
      "Epoch 331 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004767656326293945 secs\n",
      "\n",
      "Epoch 332 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046329498291015625 secs\n",
      "\n",
      "Epoch 333 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046656131744384766 secs\n",
      "\n",
      "Epoch 334 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005099058151245117 secs\n",
      "\n",
      "Saving checkpoint for epoch 335 at ./checkpoints/train/ckpt-1765\n",
      "Epoch 335 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015238761901855469 secs\n",
      "\n",
      "Epoch 336 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004567384719848633 secs\n",
      "\n",
      "Epoch 337 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046727657318115234 secs\n",
      "\n",
      "Epoch 338 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004656791687011719 secs\n",
      "\n",
      "Epoch 339 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004787445068359375 secs\n",
      "\n",
      "Saving checkpoint for epoch 340 at ./checkpoints/train/ckpt-1766\n",
      "Epoch 340 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016025066375732422 secs\n",
      "\n",
      "Epoch 341 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004633188247680664 secs\n",
      "\n",
      "Epoch 342 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004408597946166992 secs\n",
      "\n",
      "Epoch 343 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004606485366821289 secs\n",
      "\n",
      "Epoch 344 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046694278717041016 secs\n",
      "\n",
      "Saving checkpoint for epoch 345 at ./checkpoints/train/ckpt-1767\n",
      "Epoch 345 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01500082015991211 secs\n",
      "\n",
      "Epoch 346 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045354366302490234 secs\n",
      "\n",
      "Epoch 347 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004583597183227539 secs\n",
      "\n",
      "Epoch 348 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004641294479370117 secs\n",
      "\n",
      "Epoch 349 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004544258117675781 secs\n",
      "\n",
      "Saving checkpoint for epoch 350 at ./checkpoints/train/ckpt-1768\n",
      "Epoch 350 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015274286270141602 secs\n",
      "\n",
      "Epoch 351 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004781961441040039 secs\n",
      "\n",
      "Epoch 352 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0043866634368896484 secs\n",
      "\n",
      "Epoch 353 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044651031494140625 secs\n",
      "\n",
      "Epoch 354 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004616975784301758 secs\n",
      "\n",
      "Saving checkpoint for epoch 355 at ./checkpoints/train/ckpt-1769\n",
      "Epoch 355 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01543879508972168 secs\n",
      "\n",
      "Epoch 356 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004762172698974609 secs\n",
      "\n",
      "Epoch 357 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004545927047729492 secs\n",
      "\n",
      "Epoch 358 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004724264144897461 secs\n",
      "\n",
      "Epoch 359 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044133663177490234 secs\n",
      "\n",
      "Saving checkpoint for epoch 360 at ./checkpoints/train/ckpt-1770\n",
      "Epoch 360 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015485048294067383 secs\n",
      "\n",
      "Epoch 361 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004451751708984375 secs\n",
      "\n",
      "Epoch 362 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004431486129760742 secs\n",
      "\n",
      "Epoch 363 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004569292068481445 secs\n",
      "\n",
      "Epoch 364 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004520893096923828 secs\n",
      "\n",
      "Saving checkpoint for epoch 365 at ./checkpoints/train/ckpt-1771\n",
      "Epoch 365 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015146970748901367 secs\n",
      "\n",
      "Epoch 366 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004721403121948242 secs\n",
      "\n",
      "Epoch 367 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045506954193115234 secs\n",
      "\n",
      "Epoch 368 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004522800445556641 secs\n",
      "\n",
      "Epoch 369 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004652500152587891 secs\n",
      "\n",
      "Saving checkpoint for epoch 370 at ./checkpoints/train/ckpt-1772\n",
      "Epoch 370 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014848709106445312 secs\n",
      "\n",
      "Epoch 371 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045316219329833984 secs\n",
      "\n",
      "Epoch 372 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004595041275024414 secs\n",
      "\n",
      "Epoch 373 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00466609001159668 secs\n",
      "\n",
      "Epoch 374 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004545927047729492 secs\n",
      "\n",
      "Saving checkpoint for epoch 375 at ./checkpoints/train/ckpt-1773\n",
      "Epoch 375 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015062093734741211 secs\n",
      "\n",
      "Epoch 376 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0043506622314453125 secs\n",
      "\n",
      "Epoch 377 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004610538482666016 secs\n",
      "\n",
      "Epoch 378 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004303932189941406 secs\n",
      "\n",
      "Epoch 379 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004608631134033203 secs\n",
      "\n",
      "Saving checkpoint for epoch 380 at ./checkpoints/train/ckpt-1774\n",
      "Epoch 380 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015416860580444336 secs\n",
      "\n",
      "Epoch 381 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004605293273925781 secs\n",
      "\n",
      "Epoch 382 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004289865493774414 secs\n",
      "\n",
      "Epoch 383 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004390239715576172 secs\n",
      "\n",
      "Epoch 384 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00446772575378418 secs\n",
      "\n",
      "Saving checkpoint for epoch 385 at ./checkpoints/train/ckpt-1775\n",
      "Epoch 385 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014924764633178711 secs\n",
      "\n",
      "Epoch 386 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004541873931884766 secs\n",
      "\n",
      "Epoch 387 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047609806060791016 secs\n",
      "\n",
      "Epoch 388 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.006086587905883789 secs\n",
      "\n",
      "Epoch 389 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00467991828918457 secs\n",
      "\n",
      "Saving checkpoint for epoch 390 at ./checkpoints/train/ckpt-1776\n",
      "Epoch 390 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01550912857055664 secs\n",
      "\n",
      "Epoch 391 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004495859146118164 secs\n",
      "\n",
      "Epoch 392 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004622459411621094 secs\n",
      "\n",
      "Epoch 393 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004293203353881836 secs\n",
      "\n",
      "Epoch 394 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0043375492095947266 secs\n",
      "\n",
      "Saving checkpoint for epoch 395 at ./checkpoints/train/ckpt-1777\n",
      "Epoch 395 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014608144760131836 secs\n",
      "\n",
      "Epoch 396 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044596195220947266 secs\n",
      "\n",
      "Epoch 397 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004451751708984375 secs\n",
      "\n",
      "Epoch 398 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004364728927612305 secs\n",
      "\n",
      "Epoch 399 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004657268524169922 secs\n",
      "\n",
      "Saving checkpoint for epoch 400 at ./checkpoints/train/ckpt-1778\n",
      "Epoch 400 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014998197555541992 secs\n",
      "\n",
      "Epoch 401 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045697689056396484 secs\n",
      "\n",
      "Epoch 402 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004606485366821289 secs\n",
      "\n",
      "Epoch 403 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004486799240112305 secs\n",
      "\n",
      "Epoch 404 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004490852355957031 secs\n",
      "\n",
      "Saving checkpoint for epoch 405 at ./checkpoints/train/ckpt-1779\n",
      "Epoch 405 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014789104461669922 secs\n",
      "\n",
      "Epoch 406 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004526853561401367 secs\n",
      "\n",
      "Epoch 407 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004415988922119141 secs\n",
      "\n",
      "Epoch 408 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00420832633972168 secs\n",
      "\n",
      "Epoch 409 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004515409469604492 secs\n",
      "\n",
      "Saving checkpoint for epoch 410 at ./checkpoints/train/ckpt-1780\n",
      "Epoch 410 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014531612396240234 secs\n",
      "\n",
      "Epoch 411 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047702789306640625 secs\n",
      "\n",
      "Epoch 412 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004606008529663086 secs\n",
      "\n",
      "Epoch 413 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004622697830200195 secs\n",
      "\n",
      "Epoch 414 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004309177398681641 secs\n",
      "\n",
      "Saving checkpoint for epoch 415 at ./checkpoints/train/ckpt-1781\n",
      "Epoch 415 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014931917190551758 secs\n",
      "\n",
      "Epoch 416 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004870891571044922 secs\n",
      "\n",
      "Epoch 417 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004369258880615234 secs\n",
      "\n",
      "Epoch 418 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004517555236816406 secs\n",
      "\n",
      "Epoch 419 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004355907440185547 secs\n",
      "\n",
      "Saving checkpoint for epoch 420 at ./checkpoints/train/ckpt-1782\n",
      "Epoch 420 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015014410018920898 secs\n",
      "\n",
      "Epoch 421 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00448155403137207 secs\n",
      "\n",
      "Epoch 422 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004527568817138672 secs\n",
      "\n",
      "Epoch 423 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004502773284912109 secs\n",
      "\n",
      "Epoch 424 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00444793701171875 secs\n",
      "\n",
      "Saving checkpoint for epoch 425 at ./checkpoints/train/ckpt-1783\n",
      "Epoch 425 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015325069427490234 secs\n",
      "\n",
      "Epoch 426 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004651784896850586 secs\n",
      "\n",
      "Epoch 427 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004506349563598633 secs\n",
      "\n",
      "Epoch 428 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004769086837768555 secs\n",
      "\n",
      "Epoch 429 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004746913909912109 secs\n",
      "\n",
      "Saving checkpoint for epoch 430 at ./checkpoints/train/ckpt-1784\n",
      "Epoch 430 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0166475772857666 secs\n",
      "\n",
      "Epoch 431 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004935503005981445 secs\n",
      "\n",
      "Epoch 432 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00462031364440918 secs\n",
      "\n",
      "Epoch 433 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00449061393737793 secs\n",
      "\n",
      "Epoch 434 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004586458206176758 secs\n",
      "\n",
      "Saving checkpoint for epoch 435 at ./checkpoints/train/ckpt-1785\n",
      "Epoch 435 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016150951385498047 secs\n",
      "\n",
      "Epoch 436 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004906654357910156 secs\n",
      "\n",
      "Epoch 437 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047283172607421875 secs\n",
      "\n",
      "Epoch 438 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004484415054321289 secs\n",
      "\n",
      "Epoch 439 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047991275787353516 secs\n",
      "\n",
      "Saving checkpoint for epoch 440 at ./checkpoints/train/ckpt-1786\n",
      "Epoch 440 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01659679412841797 secs\n",
      "\n",
      "Epoch 441 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005006074905395508 secs\n",
      "\n",
      "Epoch 442 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0053751468658447266 secs\n",
      "\n",
      "Epoch 443 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00599217414855957 secs\n",
      "\n",
      "Epoch 444 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005450248718261719 secs\n",
      "\n",
      "Saving checkpoint for epoch 445 at ./checkpoints/train/ckpt-1787\n",
      "Epoch 445 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.017506837844848633 secs\n",
      "\n",
      "Epoch 446 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0052793025970458984 secs\n",
      "\n",
      "Epoch 447 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004908323287963867 secs\n",
      "\n",
      "Epoch 448 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004688739776611328 secs\n",
      "\n",
      "Epoch 449 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004921436309814453 secs\n",
      "\n",
      "Saving checkpoint for epoch 450 at ./checkpoints/train/ckpt-1788\n",
      "Epoch 450 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016407489776611328 secs\n",
      "\n",
      "Epoch 451 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005172252655029297 secs\n",
      "\n",
      "Epoch 452 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004873991012573242 secs\n",
      "\n",
      "Epoch 453 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004630088806152344 secs\n",
      "\n",
      "Epoch 454 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004637479782104492 secs\n",
      "\n",
      "Saving checkpoint for epoch 455 at ./checkpoints/train/ckpt-1789\n",
      "Epoch 455 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016196012496948242 secs\n",
      "\n",
      "Epoch 456 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004678249359130859 secs\n",
      "\n",
      "Epoch 457 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004573822021484375 secs\n",
      "\n",
      "Epoch 458 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004616975784301758 secs\n",
      "\n",
      "Epoch 459 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004680633544921875 secs\n",
      "\n",
      "Saving checkpoint for epoch 460 at ./checkpoints/train/ckpt-1790\n",
      "Epoch 460 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015993356704711914 secs\n",
      "\n",
      "Epoch 461 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004785776138305664 secs\n",
      "\n",
      "Epoch 462 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004631757736206055 secs\n",
      "\n",
      "Epoch 463 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046956539154052734 secs\n",
      "\n",
      "Epoch 464 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004548549652099609 secs\n",
      "\n",
      "Saving checkpoint for epoch 465 at ./checkpoints/train/ckpt-1791\n",
      "Epoch 465 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016066789627075195 secs\n",
      "\n",
      "Epoch 466 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004569292068481445 secs\n",
      "\n",
      "Epoch 467 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047223567962646484 secs\n",
      "\n",
      "Epoch 468 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00461268424987793 secs\n",
      "\n",
      "Epoch 469 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004584312438964844 secs\n",
      "\n",
      "Saving checkpoint for epoch 470 at ./checkpoints/train/ckpt-1792\n",
      "Epoch 470 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015745878219604492 secs\n",
      "\n",
      "Epoch 471 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004644870758056641 secs\n",
      "\n",
      "Epoch 472 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004592180252075195 secs\n",
      "\n",
      "Epoch 473 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00466465950012207 secs\n",
      "\n",
      "Epoch 474 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004706621170043945 secs\n",
      "\n",
      "Saving checkpoint for epoch 475 at ./checkpoints/train/ckpt-1793\n",
      "Epoch 475 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016905784606933594 secs\n",
      "\n",
      "Epoch 476 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004694938659667969 secs\n",
      "\n",
      "Epoch 477 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004720449447631836 secs\n",
      "\n",
      "Epoch 478 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004604339599609375 secs\n",
      "\n",
      "Epoch 479 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045506954193115234 secs\n",
      "\n",
      "Saving checkpoint for epoch 480 at ./checkpoints/train/ckpt-1794\n",
      "Epoch 480 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01608896255493164 secs\n",
      "\n",
      "Epoch 481 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004830360412597656 secs\n",
      "\n",
      "Epoch 482 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004813194274902344 secs\n",
      "\n",
      "Epoch 483 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004839420318603516 secs\n",
      "\n",
      "Epoch 484 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004548072814941406 secs\n",
      "\n",
      "Saving checkpoint for epoch 485 at ./checkpoints/train/ckpt-1795\n",
      "Epoch 485 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016956567764282227 secs\n",
      "\n",
      "Epoch 486 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005003213882446289 secs\n",
      "\n",
      "Epoch 487 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004665374755859375 secs\n",
      "\n",
      "Epoch 488 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046803951263427734 secs\n",
      "\n",
      "Epoch 489 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046536922454833984 secs\n",
      "\n",
      "Saving checkpoint for epoch 490 at ./checkpoints/train/ckpt-1796\n",
      "Epoch 490 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01609182357788086 secs\n",
      "\n",
      "Epoch 491 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004765033721923828 secs\n",
      "\n",
      "Epoch 492 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004723072052001953 secs\n",
      "\n",
      "Epoch 493 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004786252975463867 secs\n",
      "\n",
      "Epoch 494 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00487828254699707 secs\n",
      "\n",
      "Saving checkpoint for epoch 495 at ./checkpoints/train/ckpt-1797\n",
      "Epoch 495 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01578688621520996 secs\n",
      "\n",
      "Epoch 496 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004680633544921875 secs\n",
      "\n",
      "Epoch 497 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00464630126953125 secs\n",
      "\n",
      "Epoch 498 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046579837799072266 secs\n",
      "\n",
      "Epoch 499 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004678964614868164 secs\n",
      "\n",
      "Saving checkpoint for epoch 500 at ./checkpoints/train/ckpt-1798\n",
      "Epoch 500 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015828609466552734 secs\n",
      "\n",
      "Epoch 501 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004786491394042969 secs\n",
      "\n",
      "Epoch 502 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047452449798583984 secs\n",
      "\n",
      "Epoch 503 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004696846008300781 secs\n",
      "\n",
      "Epoch 504 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004683017730712891 secs\n",
      "\n",
      "Saving checkpoint for epoch 505 at ./checkpoints/train/ckpt-1799\n",
      "Epoch 505 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016494274139404297 secs\n",
      "\n",
      "Epoch 506 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004746913909912109 secs\n",
      "\n",
      "Epoch 507 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004643678665161133 secs\n",
      "\n",
      "Epoch 508 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00470733642578125 secs\n",
      "\n",
      "Epoch 509 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004748821258544922 secs\n",
      "\n",
      "Saving checkpoint for epoch 510 at ./checkpoints/train/ckpt-1800\n",
      "Epoch 510 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016091346740722656 secs\n",
      "\n",
      "Epoch 511 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004930734634399414 secs\n",
      "\n",
      "Epoch 512 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004712343215942383 secs\n",
      "\n",
      "Epoch 513 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004557371139526367 secs\n",
      "\n",
      "Epoch 514 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004694223403930664 secs\n",
      "\n",
      "Saving checkpoint for epoch 515 at ./checkpoints/train/ckpt-1801\n",
      "Epoch 515 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015741348266601562 secs\n",
      "\n",
      "Epoch 516 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046846866607666016 secs\n",
      "\n",
      "Epoch 517 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046269893646240234 secs\n",
      "\n",
      "Epoch 518 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0053255558013916016 secs\n",
      "\n",
      "Epoch 519 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0052149295806884766 secs\n",
      "\n",
      "Saving checkpoint for epoch 520 at ./checkpoints/train/ckpt-1802\n",
      "Epoch 520 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016070842742919922 secs\n",
      "\n",
      "Epoch 521 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004653453826904297 secs\n",
      "\n",
      "Epoch 522 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004712104797363281 secs\n",
      "\n",
      "Epoch 523 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004611015319824219 secs\n",
      "\n",
      "Epoch 524 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004876613616943359 secs\n",
      "\n",
      "Saving checkpoint for epoch 525 at ./checkpoints/train/ckpt-1803\n",
      "Epoch 525 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015922069549560547 secs\n",
      "\n",
      "Epoch 526 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004724025726318359 secs\n",
      "\n",
      "Epoch 527 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004630327224731445 secs\n",
      "\n",
      "Epoch 528 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004667758941650391 secs\n",
      "\n",
      "Epoch 529 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004620552062988281 secs\n",
      "\n",
      "Saving checkpoint for epoch 530 at ./checkpoints/train/ckpt-1804\n",
      "Epoch 530 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015977144241333008 secs\n",
      "\n",
      "Epoch 531 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00463557243347168 secs\n",
      "\n",
      "Epoch 532 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045664310455322266 secs\n",
      "\n",
      "Epoch 533 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046160221099853516 secs\n",
      "\n",
      "Epoch 534 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004527568817138672 secs\n",
      "\n",
      "Saving checkpoint for epoch 535 at ./checkpoints/train/ckpt-1805\n",
      "Epoch 535 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016216039657592773 secs\n",
      "\n",
      "Epoch 536 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004690647125244141 secs\n",
      "\n",
      "Epoch 537 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046024322509765625 secs\n",
      "\n",
      "Epoch 538 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004706621170043945 secs\n",
      "\n",
      "Epoch 539 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004488229751586914 secs\n",
      "\n",
      "Saving checkpoint for epoch 540 at ./checkpoints/train/ckpt-1806\n",
      "Epoch 540 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016846656799316406 secs\n",
      "\n",
      "Epoch 541 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004579305648803711 secs\n",
      "\n",
      "Epoch 542 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004637479782104492 secs\n",
      "\n",
      "Epoch 543 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044994354248046875 secs\n",
      "\n",
      "Epoch 544 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004536628723144531 secs\n",
      "\n",
      "Saving checkpoint for epoch 545 at ./checkpoints/train/ckpt-1807\n",
      "Epoch 545 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015397071838378906 secs\n",
      "\n",
      "Epoch 546 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004609823226928711 secs\n",
      "\n",
      "Epoch 547 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004475831985473633 secs\n",
      "\n",
      "Epoch 548 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046236515045166016 secs\n",
      "\n",
      "Epoch 549 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004518985748291016 secs\n",
      "\n",
      "Saving checkpoint for epoch 550 at ./checkpoints/train/ckpt-1808\n",
      "Epoch 550 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015462875366210938 secs\n",
      "\n",
      "Epoch 551 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047724246978759766 secs\n",
      "\n",
      "Epoch 552 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004508495330810547 secs\n",
      "\n",
      "Epoch 553 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004546403884887695 secs\n",
      "\n",
      "Epoch 554 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004712581634521484 secs\n",
      "\n",
      "Saving checkpoint for epoch 555 at ./checkpoints/train/ckpt-1809\n",
      "Epoch 555 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015316963195800781 secs\n",
      "\n",
      "Epoch 556 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004857778549194336 secs\n",
      "\n",
      "Epoch 557 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048367977142333984 secs\n",
      "\n",
      "Epoch 558 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004647970199584961 secs\n",
      "\n",
      "Epoch 559 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004759550094604492 secs\n",
      "\n",
      "Saving checkpoint for epoch 560 at ./checkpoints/train/ckpt-1810\n",
      "Epoch 560 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01571941375732422 secs\n",
      "\n",
      "Epoch 561 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004982471466064453 secs\n",
      "\n",
      "Epoch 562 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004778861999511719 secs\n",
      "\n",
      "Epoch 563 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004747867584228516 secs\n",
      "\n",
      "Epoch 564 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045702457427978516 secs\n",
      "\n",
      "Saving checkpoint for epoch 565 at ./checkpoints/train/ckpt-1811\n",
      "Epoch 565 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01610279083251953 secs\n",
      "\n",
      "Epoch 566 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004822492599487305 secs\n",
      "\n",
      "Epoch 567 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004944324493408203 secs\n",
      "\n",
      "Epoch 568 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004896402359008789 secs\n",
      "\n",
      "Epoch 569 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00494837760925293 secs\n",
      "\n",
      "Saving checkpoint for epoch 570 at ./checkpoints/train/ckpt-1812\n",
      "Epoch 570 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0157930850982666 secs\n",
      "\n",
      "Epoch 571 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004990577697753906 secs\n",
      "\n",
      "Epoch 572 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004999876022338867 secs\n",
      "\n",
      "Epoch 573 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004937648773193359 secs\n",
      "\n",
      "Epoch 574 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004899501800537109 secs\n",
      "\n",
      "Saving checkpoint for epoch 575 at ./checkpoints/train/ckpt-1813\n",
      "Epoch 575 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01571941375732422 secs\n",
      "\n",
      "Epoch 576 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004956483840942383 secs\n",
      "\n",
      "Epoch 577 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005057811737060547 secs\n",
      "\n",
      "Epoch 578 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004518747329711914 secs\n",
      "\n",
      "Epoch 579 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005049943923950195 secs\n",
      "\n",
      "Saving checkpoint for epoch 580 at ./checkpoints/train/ckpt-1814\n",
      "Epoch 580 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016263961791992188 secs\n",
      "\n",
      "Epoch 581 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004772186279296875 secs\n",
      "\n",
      "Epoch 582 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0050203800201416016 secs\n",
      "\n",
      "Epoch 583 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005223989486694336 secs\n",
      "\n",
      "Epoch 584 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0050296783447265625 secs\n",
      "\n",
      "Saving checkpoint for epoch 585 at ./checkpoints/train/ckpt-1815\n",
      "Epoch 585 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016757726669311523 secs\n",
      "\n",
      "Epoch 586 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0051305294036865234 secs\n",
      "\n",
      "Epoch 587 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005259990692138672 secs\n",
      "\n",
      "Epoch 588 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0050144195556640625 secs\n",
      "\n",
      "Epoch 589 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004792213439941406 secs\n",
      "\n",
      "Saving checkpoint for epoch 590 at ./checkpoints/train/ckpt-1816\n",
      "Epoch 590 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01616978645324707 secs\n",
      "\n",
      "Epoch 591 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045969486236572266 secs\n",
      "\n",
      "Epoch 592 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004578828811645508 secs\n",
      "\n",
      "Epoch 593 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004460573196411133 secs\n",
      "\n",
      "Epoch 594 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005012035369873047 secs\n",
      "\n",
      "Saving checkpoint for epoch 595 at ./checkpoints/train/ckpt-1817\n",
      "Epoch 595 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015440702438354492 secs\n",
      "\n",
      "Epoch 596 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004521608352661133 secs\n",
      "\n",
      "Epoch 597 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0042836666107177734 secs\n",
      "\n",
      "Epoch 598 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004668712615966797 secs\n",
      "\n",
      "Epoch 599 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004540920257568359 secs\n",
      "\n",
      "Saving checkpoint for epoch 600 at ./checkpoints/train/ckpt-1818\n",
      "Epoch 600 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0149383544921875 secs\n",
      "\n",
      "Epoch 601 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004472970962524414 secs\n",
      "\n",
      "Epoch 602 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004534721374511719 secs\n",
      "\n",
      "Epoch 603 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044803619384765625 secs\n",
      "\n",
      "Epoch 604 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00442194938659668 secs\n",
      "\n",
      "Saving checkpoint for epoch 605 at ./checkpoints/train/ckpt-1819\n",
      "Epoch 605 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015416860580444336 secs\n",
      "\n",
      "Epoch 606 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004545450210571289 secs\n",
      "\n",
      "Epoch 607 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044727325439453125 secs\n",
      "\n",
      "Epoch 608 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044667720794677734 secs\n",
      "\n",
      "Epoch 609 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044481754302978516 secs\n",
      "\n",
      "Saving checkpoint for epoch 610 at ./checkpoints/train/ckpt-1820\n",
      "Epoch 610 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015403270721435547 secs\n",
      "\n",
      "Epoch 611 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004584550857543945 secs\n",
      "\n",
      "Epoch 612 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004413127899169922 secs\n",
      "\n",
      "Epoch 613 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004559755325317383 secs\n",
      "\n",
      "Epoch 614 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004612445831298828 secs\n",
      "\n",
      "Saving checkpoint for epoch 615 at ./checkpoints/train/ckpt-1821\n",
      "Epoch 615 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015132427215576172 secs\n",
      "\n",
      "Epoch 616 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004430055618286133 secs\n",
      "\n",
      "Epoch 617 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004464626312255859 secs\n",
      "\n",
      "Epoch 618 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004424095153808594 secs\n",
      "\n",
      "Epoch 619 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004545927047729492 secs\n",
      "\n",
      "Saving checkpoint for epoch 620 at ./checkpoints/train/ckpt-1822\n",
      "Epoch 620 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014975309371948242 secs\n",
      "\n",
      "Epoch 621 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004634380340576172 secs\n",
      "\n",
      "Epoch 622 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045299530029296875 secs\n",
      "\n",
      "Epoch 623 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046100616455078125 secs\n",
      "\n",
      "Epoch 624 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0043828487396240234 secs\n",
      "\n",
      "Saving checkpoint for epoch 625 at ./checkpoints/train/ckpt-1823\n",
      "Epoch 625 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01569652557373047 secs\n",
      "\n",
      "Epoch 626 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004595756530761719 secs\n",
      "\n",
      "Epoch 627 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044863224029541016 secs\n",
      "\n",
      "Epoch 628 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046749114990234375 secs\n",
      "\n",
      "Epoch 629 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0042726993560791016 secs\n",
      "\n",
      "Saving checkpoint for epoch 630 at ./checkpoints/train/ckpt-1824\n",
      "Epoch 630 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01490473747253418 secs\n",
      "\n",
      "Epoch 631 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004373311996459961 secs\n",
      "\n",
      "Epoch 632 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004892110824584961 secs\n",
      "\n",
      "Epoch 633 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004565238952636719 secs\n",
      "\n",
      "Epoch 634 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004625082015991211 secs\n",
      "\n",
      "Saving checkpoint for epoch 635 at ./checkpoints/train/ckpt-1825\n",
      "Epoch 635 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015170812606811523 secs\n",
      "\n",
      "Epoch 636 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004595518112182617 secs\n",
      "\n",
      "Epoch 637 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004683256149291992 secs\n",
      "\n",
      "Epoch 638 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005110740661621094 secs\n",
      "\n",
      "Epoch 639 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046498775482177734 secs\n",
      "\n",
      "Saving checkpoint for epoch 640 at ./checkpoints/train/ckpt-1826\n",
      "Epoch 640 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015169382095336914 secs\n",
      "\n",
      "Epoch 641 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00453495979309082 secs\n",
      "\n",
      "Epoch 642 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045545101165771484 secs\n",
      "\n",
      "Epoch 643 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004488706588745117 secs\n",
      "\n",
      "Epoch 644 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004299163818359375 secs\n",
      "\n",
      "Saving checkpoint for epoch 645 at ./checkpoints/train/ckpt-1827\n",
      "Epoch 645 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01499485969543457 secs\n",
      "\n",
      "Epoch 646 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004398822784423828 secs\n",
      "\n",
      "Epoch 647 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00453948974609375 secs\n",
      "\n",
      "Epoch 648 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044710636138916016 secs\n",
      "\n",
      "Epoch 649 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004683971405029297 secs\n",
      "\n",
      "Saving checkpoint for epoch 650 at ./checkpoints/train/ckpt-1828\n",
      "Epoch 650 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01538395881652832 secs\n",
      "\n",
      "Epoch 651 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046236515045166016 secs\n",
      "\n",
      "Epoch 652 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004436016082763672 secs\n",
      "\n",
      "Epoch 653 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045511722564697266 secs\n",
      "\n",
      "Epoch 654 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044040679931640625 secs\n",
      "\n",
      "Saving checkpoint for epoch 655 at ./checkpoints/train/ckpt-1829\n",
      "Epoch 655 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015866994857788086 secs\n",
      "\n",
      "Epoch 656 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004525423049926758 secs\n",
      "\n",
      "Epoch 657 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004712820053100586 secs\n",
      "\n",
      "Epoch 658 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00452733039855957 secs\n",
      "\n",
      "Epoch 659 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004497051239013672 secs\n",
      "\n",
      "Saving checkpoint for epoch 660 at ./checkpoints/train/ckpt-1830\n",
      "Epoch 660 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015768766403198242 secs\n",
      "\n",
      "Epoch 661 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004619121551513672 secs\n",
      "\n",
      "Epoch 662 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004572391510009766 secs\n",
      "\n",
      "Epoch 663 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045697689056396484 secs\n",
      "\n",
      "Epoch 664 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004645824432373047 secs\n",
      "\n",
      "Saving checkpoint for epoch 665 at ./checkpoints/train/ckpt-1831\n",
      "Epoch 665 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016087770462036133 secs\n",
      "\n",
      "Epoch 666 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0050067901611328125 secs\n",
      "\n",
      "Epoch 667 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048825740814208984 secs\n",
      "\n",
      "Epoch 668 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004771709442138672 secs\n",
      "\n",
      "Epoch 669 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004796504974365234 secs\n",
      "\n",
      "Saving checkpoint for epoch 670 at ./checkpoints/train/ckpt-1832\n",
      "Epoch 670 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016077041625976562 secs\n",
      "\n",
      "Epoch 671 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004909992218017578 secs\n",
      "\n",
      "Epoch 672 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004602670669555664 secs\n",
      "\n",
      "Epoch 673 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004902362823486328 secs\n",
      "\n",
      "Epoch 674 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005003452301025391 secs\n",
      "\n",
      "Saving checkpoint for epoch 675 at ./checkpoints/train/ckpt-1833\n",
      "Epoch 675 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016475439071655273 secs\n",
      "\n",
      "Epoch 676 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0052852630615234375 secs\n",
      "\n",
      "Epoch 677 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005662202835083008 secs\n",
      "\n",
      "Epoch 678 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005414009094238281 secs\n",
      "\n",
      "Epoch 679 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005418300628662109 secs\n",
      "\n",
      "Saving checkpoint for epoch 680 at ./checkpoints/train/ckpt-1834\n",
      "Epoch 680 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.017940282821655273 secs\n",
      "\n",
      "Epoch 681 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005353689193725586 secs\n",
      "\n",
      "Epoch 682 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005327939987182617 secs\n",
      "\n",
      "Epoch 683 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005351543426513672 secs\n",
      "\n",
      "Epoch 684 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005914449691772461 secs\n",
      "\n",
      "Saving checkpoint for epoch 685 at ./checkpoints/train/ckpt-1835\n",
      "Epoch 685 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.017759323120117188 secs\n",
      "\n",
      "Epoch 686 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0051364898681640625 secs\n",
      "\n",
      "Epoch 687 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005071401596069336 secs\n",
      "\n",
      "Epoch 688 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005007743835449219 secs\n",
      "\n",
      "Epoch 689 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005051851272583008 secs\n",
      "\n",
      "Saving checkpoint for epoch 690 at ./checkpoints/train/ckpt-1836\n",
      "Epoch 690 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.018082618713378906 secs\n",
      "\n",
      "Epoch 691 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004939079284667969 secs\n",
      "\n",
      "Epoch 692 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004782438278198242 secs\n",
      "\n",
      "Epoch 693 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004726409912109375 secs\n",
      "\n",
      "Epoch 694 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004781484603881836 secs\n",
      "\n",
      "Saving checkpoint for epoch 695 at ./checkpoints/train/ckpt-1837\n",
      "Epoch 695 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016414642333984375 secs\n",
      "\n",
      "Epoch 696 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005045413970947266 secs\n",
      "\n",
      "Epoch 697 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048291683197021484 secs\n",
      "\n",
      "Epoch 698 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004815340042114258 secs\n",
      "\n",
      "Epoch 699 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004666805267333984 secs\n",
      "\n",
      "Saving checkpoint for epoch 700 at ./checkpoints/train/ckpt-1838\n",
      "Epoch 700 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016083240509033203 secs\n",
      "\n",
      "Epoch 701 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004670619964599609 secs\n",
      "\n",
      "Epoch 702 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004702329635620117 secs\n",
      "\n",
      "Epoch 703 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045719146728515625 secs\n",
      "\n",
      "Epoch 704 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004497528076171875 secs\n",
      "\n",
      "Saving checkpoint for epoch 705 at ./checkpoints/train/ckpt-1839\n",
      "Epoch 705 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015830516815185547 secs\n",
      "\n",
      "Epoch 706 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004842042922973633 secs\n",
      "\n",
      "Epoch 707 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004764080047607422 secs\n",
      "\n",
      "Epoch 708 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004693031311035156 secs\n",
      "\n",
      "Epoch 709 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004558086395263672 secs\n",
      "\n",
      "Saving checkpoint for epoch 710 at ./checkpoints/train/ckpt-1840\n",
      "Epoch 710 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016240358352661133 secs\n",
      "\n",
      "Epoch 711 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005815744400024414 secs\n",
      "\n",
      "Epoch 712 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005361080169677734 secs\n",
      "\n",
      "Epoch 713 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005176067352294922 secs\n",
      "\n",
      "Epoch 714 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0051686763763427734 secs\n",
      "\n",
      "Saving checkpoint for epoch 715 at ./checkpoints/train/ckpt-1841\n",
      "Epoch 715 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.017225265502929688 secs\n",
      "\n",
      "Epoch 716 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004914283752441406 secs\n",
      "\n",
      "Epoch 717 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005311250686645508 secs\n",
      "\n",
      "Epoch 718 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005209684371948242 secs\n",
      "\n",
      "Epoch 719 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0052683353424072266 secs\n",
      "\n",
      "Saving checkpoint for epoch 720 at ./checkpoints/train/ckpt-1842\n",
      "Epoch 720 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.018047094345092773 secs\n",
      "\n",
      "Epoch 721 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004705905914306641 secs\n",
      "\n",
      "Epoch 722 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004697322845458984 secs\n",
      "\n",
      "Epoch 723 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004683971405029297 secs\n",
      "\n",
      "Epoch 724 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004654645919799805 secs\n",
      "\n",
      "Saving checkpoint for epoch 725 at ./checkpoints/train/ckpt-1843\n",
      "Epoch 725 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015859365463256836 secs\n",
      "\n",
      "Epoch 726 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004742622375488281 secs\n",
      "\n",
      "Epoch 727 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004794120788574219 secs\n",
      "\n",
      "Epoch 728 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004614591598510742 secs\n",
      "\n",
      "Epoch 729 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004635334014892578 secs\n",
      "\n",
      "Saving checkpoint for epoch 730 at ./checkpoints/train/ckpt-1844\n",
      "Epoch 730 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015836715698242188 secs\n",
      "\n",
      "Epoch 731 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00483250617980957 secs\n",
      "\n",
      "Epoch 732 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004674434661865234 secs\n",
      "\n",
      "Epoch 733 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004633188247680664 secs\n",
      "\n",
      "Epoch 734 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004959821701049805 secs\n",
      "\n",
      "Saving checkpoint for epoch 735 at ./checkpoints/train/ckpt-1845\n",
      "Epoch 735 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016044139862060547 secs\n",
      "\n",
      "Epoch 736 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00484919548034668 secs\n",
      "\n",
      "Epoch 737 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004786014556884766 secs\n",
      "\n",
      "Epoch 738 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00454258918762207 secs\n",
      "\n",
      "Epoch 739 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004607200622558594 secs\n",
      "\n",
      "Saving checkpoint for epoch 740 at ./checkpoints/train/ckpt-1846\n",
      "Epoch 740 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016708850860595703 secs\n",
      "\n",
      "Epoch 741 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046498775482177734 secs\n",
      "\n",
      "Epoch 742 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004849672317504883 secs\n",
      "\n",
      "Epoch 743 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004762172698974609 secs\n",
      "\n",
      "Epoch 744 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004576921463012695 secs\n",
      "\n",
      "Saving checkpoint for epoch 745 at ./checkpoints/train/ckpt-1847\n",
      "Epoch 745 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015988588333129883 secs\n",
      "\n",
      "Epoch 746 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004766941070556641 secs\n",
      "\n",
      "Epoch 747 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004935741424560547 secs\n",
      "\n",
      "Epoch 748 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047724246978759766 secs\n",
      "\n",
      "Epoch 749 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004610776901245117 secs\n",
      "\n",
      "Saving checkpoint for epoch 750 at ./checkpoints/train/ckpt-1848\n",
      "Epoch 750 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015483617782592773 secs\n",
      "\n",
      "Epoch 751 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004622936248779297 secs\n",
      "\n",
      "Epoch 752 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046234130859375 secs\n",
      "\n",
      "Epoch 753 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045511722564697266 secs\n",
      "\n",
      "Epoch 754 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00456690788269043 secs\n",
      "\n",
      "Saving checkpoint for epoch 755 at ./checkpoints/train/ckpt-1849\n",
      "Epoch 755 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015395879745483398 secs\n",
      "\n",
      "Epoch 756 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046918392181396484 secs\n",
      "\n",
      "Epoch 757 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004603147506713867 secs\n",
      "\n",
      "Epoch 758 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004567861557006836 secs\n",
      "\n",
      "Epoch 759 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004664421081542969 secs\n",
      "\n",
      "Saving checkpoint for epoch 760 at ./checkpoints/train/ckpt-1850\n",
      "Epoch 760 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014785289764404297 secs\n",
      "\n",
      "Epoch 761 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046100616455078125 secs\n",
      "\n",
      "Epoch 762 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046041011810302734 secs\n",
      "\n",
      "Epoch 763 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004415750503540039 secs\n",
      "\n",
      "Epoch 764 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004437685012817383 secs\n",
      "\n",
      "Saving checkpoint for epoch 765 at ./checkpoints/train/ckpt-1851\n",
      "Epoch 765 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015257835388183594 secs\n",
      "\n",
      "Epoch 766 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004580497741699219 secs\n",
      "\n",
      "Epoch 767 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004630088806152344 secs\n",
      "\n",
      "Epoch 768 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004928112030029297 secs\n",
      "\n",
      "Epoch 769 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004751920700073242 secs\n",
      "\n",
      "Saving checkpoint for epoch 770 at ./checkpoints/train/ckpt-1852\n",
      "Epoch 770 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015417098999023438 secs\n",
      "\n",
      "Epoch 771 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004633665084838867 secs\n",
      "\n",
      "Epoch 772 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004842042922973633 secs\n",
      "\n",
      "Epoch 773 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004860877990722656 secs\n",
      "\n",
      "Epoch 774 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004642486572265625 secs\n",
      "\n",
      "Saving checkpoint for epoch 775 at ./checkpoints/train/ckpt-1853\n",
      "Epoch 775 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015680313110351562 secs\n",
      "\n",
      "Epoch 776 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004839181900024414 secs\n",
      "\n",
      "Epoch 777 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048274993896484375 secs\n",
      "\n",
      "Epoch 778 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047454833984375 secs\n",
      "\n",
      "Epoch 779 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004377841949462891 secs\n",
      "\n",
      "Saving checkpoint for epoch 780 at ./checkpoints/train/ckpt-1854\n",
      "Epoch 780 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01464390754699707 secs\n",
      "\n",
      "Epoch 781 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00462651252746582 secs\n",
      "\n",
      "Epoch 782 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004494667053222656 secs\n",
      "\n",
      "Epoch 783 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004652261734008789 secs\n",
      "\n",
      "Epoch 784 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045583248138427734 secs\n",
      "\n",
      "Saving checkpoint for epoch 785 at ./checkpoints/train/ckpt-1855\n",
      "Epoch 785 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015079021453857422 secs\n",
      "\n",
      "Epoch 786 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0049915313720703125 secs\n",
      "\n",
      "Epoch 787 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00464320182800293 secs\n",
      "\n",
      "Epoch 788 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004571199417114258 secs\n",
      "\n",
      "Epoch 789 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004586219787597656 secs\n",
      "\n",
      "Saving checkpoint for epoch 790 at ./checkpoints/train/ckpt-1856\n",
      "Epoch 790 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01551508903503418 secs\n",
      "\n",
      "Epoch 791 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045413970947265625 secs\n",
      "\n",
      "Epoch 792 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004651308059692383 secs\n",
      "\n",
      "Epoch 793 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004465341567993164 secs\n",
      "\n",
      "Epoch 794 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004234790802001953 secs\n",
      "\n",
      "Saving checkpoint for epoch 795 at ./checkpoints/train/ckpt-1857\n",
      "Epoch 795 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015486478805541992 secs\n",
      "\n",
      "Epoch 796 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046672821044921875 secs\n",
      "\n",
      "Epoch 797 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046498775482177734 secs\n",
      "\n",
      "Epoch 798 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00479435920715332 secs\n",
      "\n",
      "Epoch 799 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004755973815917969 secs\n",
      "\n",
      "Saving checkpoint for epoch 800 at ./checkpoints/train/ckpt-1858\n",
      "Epoch 800 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015363693237304688 secs\n",
      "\n",
      "Epoch 801 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004852294921875 secs\n",
      "\n",
      "Epoch 802 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004716157913208008 secs\n",
      "\n",
      "Epoch 803 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045239925384521484 secs\n",
      "\n",
      "Epoch 804 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004705190658569336 secs\n",
      "\n",
      "Saving checkpoint for epoch 805 at ./checkpoints/train/ckpt-1859\n",
      "Epoch 805 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014759302139282227 secs\n",
      "\n",
      "Epoch 806 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004563093185424805 secs\n",
      "\n",
      "Epoch 807 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004507780075073242 secs\n",
      "\n",
      "Epoch 808 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004519939422607422 secs\n",
      "\n",
      "Epoch 809 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004708528518676758 secs\n",
      "\n",
      "Saving checkpoint for epoch 810 at ./checkpoints/train/ckpt-1860\n",
      "Epoch 810 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015254497528076172 secs\n",
      "\n",
      "Epoch 811 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004821300506591797 secs\n",
      "\n",
      "Epoch 812 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004403352737426758 secs\n",
      "\n",
      "Epoch 813 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004397153854370117 secs\n",
      "\n",
      "Epoch 814 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0043773651123046875 secs\n",
      "\n",
      "Saving checkpoint for epoch 815 at ./checkpoints/train/ckpt-1861\n",
      "Epoch 815 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015059709548950195 secs\n",
      "\n",
      "Epoch 816 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004432201385498047 secs\n",
      "\n",
      "Epoch 817 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045163631439208984 secs\n",
      "\n",
      "Epoch 818 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0043201446533203125 secs\n",
      "\n",
      "Epoch 819 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004758596420288086 secs\n",
      "\n",
      "Saving checkpoint for epoch 820 at ./checkpoints/train/ckpt-1862\n",
      "Epoch 820 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016416072845458984 secs\n",
      "\n",
      "Epoch 821 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047910213470458984 secs\n",
      "\n",
      "Epoch 822 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046291351318359375 secs\n",
      "\n",
      "Epoch 823 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004645824432373047 secs\n",
      "\n",
      "Epoch 824 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004457235336303711 secs\n",
      "\n",
      "Saving checkpoint for epoch 825 at ./checkpoints/train/ckpt-1863\n",
      "Epoch 825 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01504373550415039 secs\n",
      "\n",
      "Epoch 826 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004429340362548828 secs\n",
      "\n",
      "Epoch 827 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004917621612548828 secs\n",
      "\n",
      "Epoch 828 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005169868469238281 secs\n",
      "\n",
      "Epoch 829 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005236148834228516 secs\n",
      "\n",
      "Saving checkpoint for epoch 830 at ./checkpoints/train/ckpt-1864\n",
      "Epoch 830 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01739025115966797 secs\n",
      "\n",
      "Epoch 831 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005231618881225586 secs\n",
      "\n",
      "Epoch 832 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004906177520751953 secs\n",
      "\n",
      "Epoch 833 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004518032073974609 secs\n",
      "\n",
      "Epoch 834 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004629611968994141 secs\n",
      "\n",
      "Saving checkpoint for epoch 835 at ./checkpoints/train/ckpt-1865\n",
      "Epoch 835 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015702486038208008 secs\n",
      "\n",
      "Epoch 836 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004582643508911133 secs\n",
      "\n",
      "Epoch 837 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004754543304443359 secs\n",
      "\n",
      "Epoch 838 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047016143798828125 secs\n",
      "\n",
      "Epoch 839 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004569530487060547 secs\n",
      "\n",
      "Saving checkpoint for epoch 840 at ./checkpoints/train/ckpt-1866\n",
      "Epoch 840 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015065431594848633 secs\n",
      "\n",
      "Epoch 841 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005067110061645508 secs\n",
      "\n",
      "Epoch 842 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005289554595947266 secs\n",
      "\n",
      "Epoch 843 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004563808441162109 secs\n",
      "\n",
      "Epoch 844 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004532337188720703 secs\n",
      "\n",
      "Saving checkpoint for epoch 845 at ./checkpoints/train/ckpt-1867\n",
      "Epoch 845 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015531063079833984 secs\n",
      "\n",
      "Epoch 846 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004387378692626953 secs\n",
      "\n",
      "Epoch 847 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004683494567871094 secs\n",
      "\n",
      "Epoch 848 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004408121109008789 secs\n",
      "\n",
      "Epoch 849 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004805803298950195 secs\n",
      "\n",
      "Saving checkpoint for epoch 850 at ./checkpoints/train/ckpt-1868\n",
      "Epoch 850 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01585984230041504 secs\n",
      "\n",
      "Epoch 851 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004847288131713867 secs\n",
      "\n",
      "Epoch 852 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004506587982177734 secs\n",
      "\n",
      "Epoch 853 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005095243453979492 secs\n",
      "\n",
      "Epoch 854 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048427581787109375 secs\n",
      "\n",
      "Saving checkpoint for epoch 855 at ./checkpoints/train/ckpt-1869\n",
      "Epoch 855 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01629948616027832 secs\n",
      "\n",
      "Epoch 856 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047152042388916016 secs\n",
      "\n",
      "Epoch 857 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004614591598510742 secs\n",
      "\n",
      "Epoch 858 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045871734619140625 secs\n",
      "\n",
      "Epoch 859 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004851341247558594 secs\n",
      "\n",
      "Saving checkpoint for epoch 860 at ./checkpoints/train/ckpt-1870\n",
      "Epoch 860 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016376972198486328 secs\n",
      "\n",
      "Epoch 861 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004789590835571289 secs\n",
      "\n",
      "Epoch 862 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004612445831298828 secs\n",
      "\n",
      "Epoch 863 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004953145980834961 secs\n",
      "\n",
      "Epoch 864 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004653453826904297 secs\n",
      "\n",
      "Saving checkpoint for epoch 865 at ./checkpoints/train/ckpt-1871\n",
      "Epoch 865 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015554189682006836 secs\n",
      "\n",
      "Epoch 866 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004664897918701172 secs\n",
      "\n",
      "Epoch 867 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004713296890258789 secs\n",
      "\n",
      "Epoch 868 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004582881927490234 secs\n",
      "\n",
      "Epoch 869 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046253204345703125 secs\n",
      "\n",
      "Saving checkpoint for epoch 870 at ./checkpoints/train/ckpt-1872\n",
      "Epoch 870 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014752626419067383 secs\n",
      "\n",
      "Epoch 871 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004456758499145508 secs\n",
      "\n",
      "Epoch 872 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00452876091003418 secs\n",
      "\n",
      "Epoch 873 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046863555908203125 secs\n",
      "\n",
      "Epoch 874 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004553556442260742 secs\n",
      "\n",
      "Saving checkpoint for epoch 875 at ./checkpoints/train/ckpt-1873\n",
      "Epoch 875 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015510797500610352 secs\n",
      "\n",
      "Epoch 876 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004650592803955078 secs\n",
      "\n",
      "Epoch 877 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004517078399658203 secs\n",
      "\n",
      "Epoch 878 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004399776458740234 secs\n",
      "\n",
      "Epoch 879 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004505634307861328 secs\n",
      "\n",
      "Saving checkpoint for epoch 880 at ./checkpoints/train/ckpt-1874\n",
      "Epoch 880 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015500545501708984 secs\n",
      "\n",
      "Epoch 881 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004535675048828125 secs\n",
      "\n",
      "Epoch 882 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004447221755981445 secs\n",
      "\n",
      "Epoch 883 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004309415817260742 secs\n",
      "\n",
      "Epoch 884 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004697084426879883 secs\n",
      "\n",
      "Saving checkpoint for epoch 885 at ./checkpoints/train/ckpt-1875\n",
      "Epoch 885 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015284299850463867 secs\n",
      "\n",
      "Epoch 886 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00450587272644043 secs\n",
      "\n",
      "Epoch 887 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004682302474975586 secs\n",
      "\n",
      "Epoch 888 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004774570465087891 secs\n",
      "\n",
      "Epoch 889 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004665374755859375 secs\n",
      "\n",
      "Saving checkpoint for epoch 890 at ./checkpoints/train/ckpt-1876\n",
      "Epoch 890 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01600956916809082 secs\n",
      "\n",
      "Epoch 891 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004681825637817383 secs\n",
      "\n",
      "Epoch 892 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004537820816040039 secs\n",
      "\n",
      "Epoch 893 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004547119140625 secs\n",
      "\n",
      "Epoch 894 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004622459411621094 secs\n",
      "\n",
      "Saving checkpoint for epoch 895 at ./checkpoints/train/ckpt-1877\n",
      "Epoch 895 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015577077865600586 secs\n",
      "\n",
      "Epoch 896 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00477910041809082 secs\n",
      "\n",
      "Epoch 897 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004354715347290039 secs\n",
      "\n",
      "Epoch 898 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004595756530761719 secs\n",
      "\n",
      "Epoch 899 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004330873489379883 secs\n",
      "\n",
      "Saving checkpoint for epoch 900 at ./checkpoints/train/ckpt-1878\n",
      "Epoch 900 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015321969985961914 secs\n",
      "\n",
      "Epoch 901 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004639148712158203 secs\n",
      "\n",
      "Epoch 902 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004613399505615234 secs\n",
      "\n",
      "Epoch 903 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004670858383178711 secs\n",
      "\n",
      "Epoch 904 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004578113555908203 secs\n",
      "\n",
      "Saving checkpoint for epoch 905 at ./checkpoints/train/ckpt-1879\n",
      "Epoch 905 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0152587890625 secs\n",
      "\n",
      "Epoch 906 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004614830017089844 secs\n",
      "\n",
      "Epoch 907 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004496335983276367 secs\n",
      "\n",
      "Epoch 908 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004655122756958008 secs\n",
      "\n",
      "Epoch 909 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004445075988769531 secs\n",
      "\n",
      "Saving checkpoint for epoch 910 at ./checkpoints/train/ckpt-1880\n",
      "Epoch 910 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014954328536987305 secs\n",
      "\n",
      "Epoch 911 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004616975784301758 secs\n",
      "\n",
      "Epoch 912 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004639863967895508 secs\n",
      "\n",
      "Epoch 913 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004845857620239258 secs\n",
      "\n",
      "Epoch 914 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004695892333984375 secs\n",
      "\n",
      "Saving checkpoint for epoch 915 at ./checkpoints/train/ckpt-1881\n",
      "Epoch 915 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015403509140014648 secs\n",
      "\n",
      "Epoch 916 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0047092437744140625 secs\n",
      "\n",
      "Epoch 917 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004788398742675781 secs\n",
      "\n",
      "Epoch 918 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004651308059692383 secs\n",
      "\n",
      "Epoch 919 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004700660705566406 secs\n",
      "\n",
      "Saving checkpoint for epoch 920 at ./checkpoints/train/ckpt-1882\n",
      "Epoch 920 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01550602912902832 secs\n",
      "\n",
      "Epoch 921 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004632234573364258 secs\n",
      "\n",
      "Epoch 922 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004485368728637695 secs\n",
      "\n",
      "Epoch 923 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004842996597290039 secs\n",
      "\n",
      "Epoch 924 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004721403121948242 secs\n",
      "\n",
      "Saving checkpoint for epoch 925 at ./checkpoints/train/ckpt-1883\n",
      "Epoch 925 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015362977981567383 secs\n",
      "\n",
      "Epoch 926 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004802703857421875 secs\n",
      "\n",
      "Epoch 927 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004576206207275391 secs\n",
      "\n",
      "Epoch 928 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045185089111328125 secs\n",
      "\n",
      "Epoch 929 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004626750946044922 secs\n",
      "\n",
      "Saving checkpoint for epoch 930 at ./checkpoints/train/ckpt-1884\n",
      "Epoch 930 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015360355377197266 secs\n",
      "\n",
      "Epoch 931 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004792928695678711 secs\n",
      "\n",
      "Epoch 932 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004646778106689453 secs\n",
      "\n",
      "Epoch 933 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046384334564208984 secs\n",
      "\n",
      "Epoch 934 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046844482421875 secs\n",
      "\n",
      "Saving checkpoint for epoch 935 at ./checkpoints/train/ckpt-1885\n",
      "Epoch 935 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014961719512939453 secs\n",
      "\n",
      "Epoch 936 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004602670669555664 secs\n",
      "\n",
      "Epoch 937 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004486083984375 secs\n",
      "\n",
      "Epoch 938 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004528045654296875 secs\n",
      "\n",
      "Epoch 939 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004509925842285156 secs\n",
      "\n",
      "Saving checkpoint for epoch 940 at ./checkpoints/train/ckpt-1886\n",
      "Epoch 940 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016098976135253906 secs\n",
      "\n",
      "Epoch 941 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004561662673950195 secs\n",
      "\n",
      "Epoch 942 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004610538482666016 secs\n",
      "\n",
      "Epoch 943 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004611015319824219 secs\n",
      "\n",
      "Epoch 944 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00466465950012207 secs\n",
      "\n",
      "Saving checkpoint for epoch 945 at ./checkpoints/train/ckpt-1887\n",
      "Epoch 945 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015232324600219727 secs\n",
      "\n",
      "Epoch 946 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048215389251708984 secs\n",
      "\n",
      "Epoch 947 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004400491714477539 secs\n",
      "\n",
      "Epoch 948 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004602670669555664 secs\n",
      "\n",
      "Epoch 949 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044708251953125 secs\n",
      "\n",
      "Saving checkpoint for epoch 950 at ./checkpoints/train/ckpt-1888\n",
      "Epoch 950 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015219449996948242 secs\n",
      "\n",
      "Epoch 951 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004649639129638672 secs\n",
      "\n",
      "Epoch 952 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004557132720947266 secs\n",
      "\n",
      "Epoch 953 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004596710205078125 secs\n",
      "\n",
      "Epoch 954 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00469207763671875 secs\n",
      "\n",
      "Saving checkpoint for epoch 955 at ./checkpoints/train/ckpt-1889\n",
      "Epoch 955 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015659332275390625 secs\n",
      "\n",
      "Epoch 956 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004510164260864258 secs\n",
      "\n",
      "Epoch 957 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004461765289306641 secs\n",
      "\n",
      "Epoch 958 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004746675491333008 secs\n",
      "\n",
      "Epoch 959 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004438638687133789 secs\n",
      "\n",
      "Saving checkpoint for epoch 960 at ./checkpoints/train/ckpt-1890\n",
      "Epoch 960 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015050172805786133 secs\n",
      "\n",
      "Epoch 961 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045282840728759766 secs\n",
      "\n",
      "Epoch 962 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045680999755859375 secs\n",
      "\n",
      "Epoch 963 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046045780181884766 secs\n",
      "\n",
      "Epoch 964 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004561185836791992 secs\n",
      "\n",
      "Saving checkpoint for epoch 965 at ./checkpoints/train/ckpt-1891\n",
      "Epoch 965 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0147857666015625 secs\n",
      "\n",
      "Epoch 966 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004525184631347656 secs\n",
      "\n",
      "Epoch 967 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005122184753417969 secs\n",
      "\n",
      "Epoch 968 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045871734619140625 secs\n",
      "\n",
      "Epoch 969 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004483699798583984 secs\n",
      "\n",
      "Saving checkpoint for epoch 970 at ./checkpoints/train/ckpt-1892\n",
      "Epoch 970 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014672279357910156 secs\n",
      "\n",
      "Epoch 971 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004511117935180664 secs\n",
      "\n",
      "Epoch 972 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004275321960449219 secs\n",
      "\n",
      "Epoch 973 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0044443607330322266 secs\n",
      "\n",
      "Epoch 974 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004470348358154297 secs\n",
      "\n",
      "Saving checkpoint for epoch 975 at ./checkpoints/train/ckpt-1893\n",
      "Epoch 975 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.016000986099243164 secs\n",
      "\n",
      "Epoch 976 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004702329635620117 secs\n",
      "\n",
      "Epoch 977 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004613637924194336 secs\n",
      "\n",
      "Epoch 978 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004809379577636719 secs\n",
      "\n",
      "Epoch 979 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004702329635620117 secs\n",
      "\n",
      "Saving checkpoint for epoch 980 at ./checkpoints/train/ckpt-1894\n",
      "Epoch 980 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01728653907775879 secs\n",
      "\n",
      "Epoch 981 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0048563480377197266 secs\n",
      "\n",
      "Epoch 982 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004502773284912109 secs\n",
      "\n",
      "Epoch 983 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004591941833496094 secs\n",
      "\n",
      "Epoch 984 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004808664321899414 secs\n",
      "\n",
      "Saving checkpoint for epoch 985 at ./checkpoints/train/ckpt-1895\n",
      "Epoch 985 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.014862060546875 secs\n",
      "\n",
      "Epoch 986 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045506954193115234 secs\n",
      "\n",
      "Epoch 987 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004458904266357422 secs\n",
      "\n",
      "Epoch 988 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004360675811767578 secs\n",
      "\n",
      "Epoch 989 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.00448155403137207 secs\n",
      "\n",
      "Saving checkpoint for epoch 990 at ./checkpoints/train/ckpt-1896\n",
      "Epoch 990 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015263080596923828 secs\n",
      "\n",
      "Epoch 991 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004436969757080078 secs\n",
      "\n",
      "Epoch 992 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045359134674072266 secs\n",
      "\n",
      "Epoch 993 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004671573638916016 secs\n",
      "\n",
      "Epoch 994 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.004783153533935547 secs\n",
      "\n",
      "Saving checkpoint for epoch 995 at ./checkpoints/train/ckpt-1897\n",
      "Epoch 995 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.01512598991394043 secs\n",
      "\n",
      "Epoch 996 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0050525665283203125 secs\n",
      "\n",
      "Epoch 997 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.005039691925048828 secs\n",
      "\n",
      "Epoch 998 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0046346187591552734 secs\n",
      "\n",
      "Epoch 999 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.0045931339263916016 secs\n",
      "\n",
      "Saving checkpoint for epoch 1000 at ./checkpoints/train/ckpt-1898\n",
      "Epoch 1000 Loss 0.0000 Accuracy 0.0000\n",
      "Time taken for 1 epoch: 0.015655517578125 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(dataset_train):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "#   for (batch, (inp, tar)) in enumerate(dataset_test):\n",
    "#     test_step(inp, tar)\n",
    "\n",
    "#     if batch % 50 == 0:\n",
    "#       print ('Epoch {} Batch {} Test_Loss {:.4f} Test_Accuracy {:.4f}'.format(\n",
    "#               epoch + 1, batch, test_loss.result(), test_accuracy.result()))      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "#   print ('Epoch {} Test_Loss {:.4f} Test_Accuracy {:.4f}'.format(epoch + 1, \n",
    "#                                                 test_loss.result(), \n",
    "#                                                 test_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QfcsSWswSdGV"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6APsFrgImLW"
   },
   "source": [
    "The following steps are used for evaluation:\n",
    "\n",
    "* Encode the input sentence using the Portuguese tokenizer (`tokenizer_pt`). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.\n",
    "* The decoder input is the `start token == tokenizer_en.vocab_size`.\n",
    "* Calculate the padding masks and the look ahead masks.\n",
    "* The `decoder` then outputs the predictions by looking at the `encoder output` and its own output (self-attention).\n",
    "* Select the last word and calculate the argmax of that.\n",
    "* Concatentate the predicted word to the decoder input as pass it to the decoder.\n",
    "* In this approach, the decoder predicts the next word based on the previous words it predicted.\n",
    "\n",
    "Note: The model used here has less capacity to keep the example relatively faster so the predictions maybe less right. To reproduce the results in the paper, use the entire dataset and base transformer model or transformer XL, by changing the hyperparameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5buvMlnvyrFm"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  start_token = [tokenizer_pt.vocab_size]\n",
    "  end_token = [tokenizer_pt.vocab_size + 1]\n",
    "  \n",
    "  # inp sentence is portuguese, hence adding the start and end token\n",
    "  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "  decoder_input = [tokenizer_en.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "  for i in range(MAX_LENGTH):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if predicted_id == tokenizer_en.vocab_size+1:\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CN-BV43FMBej"
   },
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "  fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "  sentence = tokenizer_pt.encode(sentence)\n",
    "  \n",
    "  attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "  for head in range(attention.shape[0]):\n",
    "    ax = fig.add_subplot(2, 4, head+1)\n",
    "    \n",
    "    # plot the attention weights\n",
    "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 10}\n",
    "    \n",
    "    ax.set_xticks(range(len(sentence)+2))\n",
    "    ax.set_yticks(range(len(result)))\n",
    "    \n",
    "    ax.set_ylim(len(result)-1.5, -0.5)\n",
    "        \n",
    "    ax.set_xticklabels(\n",
    "        ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "        fontdict=fontdict, rotation=90)\n",
    "    \n",
    "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                        if i < tokenizer_en.vocab_size], \n",
    "                       fontdict=fontdict)\n",
    "    \n",
    "    ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lU2_yG_vBGza"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "  result, attention_weights = evaluate(sentence)\n",
    "  \n",
    "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "  if plot:\n",
    "    plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YsxrAlvFG8SZ"
   },
   "outputs": [],
   "source": [
    "translate(\"este Ã© um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EH5y_aqI4t1"
   },
   "outputs": [],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J-hVCTSUMlkb"
   },
   "outputs": [],
   "source": [
    "translate(\"vou entÃ£o muito rapidamente partilhar convosco algumas histÃ³rias de algumas coisas mÃ¡gicas que aconteceram.\")\n",
    "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MxkSZvz0jX"
   },
   "source": [
    "You can pass different layers and attention blocks of the decoder to the `plot` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-kFyiOLH0xg"
   },
   "outputs": [],
   "source": [
    "translate(\"este Ã© o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')\n",
    "print (\"Real translation: this is the first book i've ever done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqQ1fIsLwkGE"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned about positional encoding, multi-head attention, the importance of masking and how to create a transformer.\n",
    "\n",
    "Try using a different dataset to train the transformer. You can also create the base transformer or transformer XL by changing the hyperparameters above. You can also use the layers defined here to create [BERT](https://arxiv.org/abs/1810.04805) and train state of the art models. Futhermore, you can implement beam search to get better predictions."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "s_qNSzzyaCbD"
   ],
   "name": "transformer.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0000f58d16984513bb8360fdbb4c854f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "018ea7669b3848058732a0cafea345d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c2332acd69994cf89ead0f1f51cce5c6",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f5601b1f66a24cbd807f5af49163fa9a",
       "value": 1
      }
     },
     "10612a02a0e74f10bc7c64a3f2db6922": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "133eb1b8177c4465a4301f7e6f969a7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Extraction completed...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_82555c4cf9ee4d02b7bac7203cd91244",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6f44216295484994b1f9e901c105b356",
       "value": 1
      }
     },
     "1477ac5d37344d4fa87311ec1d8f55df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "19d957d24aee47febac720c7e04fb424": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "1db86266e7174fd1a8dd356ff2e1ecbb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "20df8ac98b00405f8efde1904be9cf91": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_98a4080094d94fe3a19c330495a29cc5",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_3598a0b4431840248a6d8d3bd554b10a",
       "value": " 1803/0 [00:00&lt;00:00, 5370.39 examples/s]"
      }
     },
     "213499c03b4940809bd72db4968473ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "22d9b912b79a42559841edec84a970a0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28fc39deaa1e4981833404648b81f646": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_609f8e5e2f3e4c81b1caff53fd6527dd",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_a298c0f1c2fd4d30a9c25e46c9df8a14",
       "value": " 1193/0 [00:00&lt;00:00, 5167.71 examples/s]"
      }
     },
     "3535c3c62500455e9a7b5e04cea77739": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3598a0b4431840248a6d8d3bd554b10a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "37f231fe2bb34a7ba1f81f8648d32237": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3842bccc315f4d6e87c3ef50a48eac3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "3a66b3fb48a7453091d21b811a322c92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c9bfaf976a04ef8ba94057cb32ea647": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "3e5364b20c14408e99de045c5a123e4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_58ecb44dfd7e493d977febe14e534f31",
        "IPY_MODEL_6d719de843144dd9a747c14cb2e9c77a"
       ],
       "layout": "IPY_MODEL_1db86266e7174fd1a8dd356ff2e1ecbb"
      }
     },
     "4358cd37d09b410a80177daa469686c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4a8334cfdd474895844c3ebaf264e2a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "4f226da6604e43b3980fba7a42f7014d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "50dd6747f2af4b6887d5fde63da70543": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9627a0e9f85841acb622d3cbe9fdd6fc",
        "IPY_MODEL_b59759fd057c445187cd446224719b8b"
       ],
       "layout": "IPY_MODEL_faf87e478e3d48fa87a6625599a02f77"
      }
     },
     "529f27fe4fae497da05b2ea7480462ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5659253e6beb422aab593f4179e00af0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4358cd37d09b410a80177daa469686c5",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_7db3939f5b7f4436b5cf80afacc0f4f9",
       "value": " 21167/51785 [00:00&lt;00:00, 211667.68 examples/s]"
      }
     },
     "56d15c5c38e54744a7409698fe1c8397": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_66d5cc3d31784e7a87c3b011f53e41ed",
        "IPY_MODEL_28fc39deaa1e4981833404648b81f646"
       ],
       "layout": "IPY_MODEL_b5bb4bc1c0684058b16accaab49e12a9"
      }
     },
     "5756fa8c040146ed991c2a9f1bc476bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3a66b3fb48a7453091d21b811a322c92",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_eeb8316d3551414ab4f9349da201a9f2",
       "value": " 124/124 [00:13&lt;00:00,  9.13 MiB/s]"
      }
     },
     "58ecb44dfd7e493d977febe14e534f31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3535c3c62500455e9a7b5e04cea77739",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3c9bfaf976a04ef8ba94057cb32ea647",
       "value": 1
      }
     },
     "5fc9be90025243d4bff72c617827ad98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_faee87968e434f409848bbaf5da8f553",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_37f231fe2bb34a7ba1f81f8648d32237",
       "value": " 0/1193 [00:00&lt;?, ? examples/s]"
      }
     },
     "609f8e5e2f3e4c81b1caff53fd6527dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "66d5cc3d31784e7a87c3b011f53e41ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1477ac5d37344d4fa87311ec1d8f55df",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f844670eef6a4f01bc38d3f883c2714e",
       "value": 1
      }
     },
     "673a361778ab49938cf7b50719f135d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6ce68526eb264938b0b5464bf91c5cc5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6d719de843144dd9a747c14cb2e9c77a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_10612a02a0e74f10bc7c64a3f2db6922",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_bafe2bb292834c0c9e01ecc45db673d4",
       "value": " 51785/0 [00:09&lt;00:00, 5900.99 examples/s]"
      }
     },
     "6f44216295484994b1f9e901c105b356": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "747ec4c3baf3496ea2b77bc4a1ed51fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a009f5d170f488c8a24b6bf76027a8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": " 41%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0000f58d16984513bb8360fdbb4c854f",
       "max": 51785,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4a8334cfdd474895844c3ebaf264e2a3",
       "value": 21167
      }
     },
     "7d2bfac263f54042ae0833b496ea4301": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Dl Size...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_529f27fe4fae497da05b2ea7480462ff",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_19d957d24aee47febac720c7e04fb424",
       "value": 1
      }
     },
     "7db3939f5b7f4436b5cf80afacc0f4f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8028b2718d584bf8bce33ca6cc53b536": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7d2bfac263f54042ae0833b496ea4301",
        "IPY_MODEL_5756fa8c040146ed991c2a9f1bc476bc"
       ],
       "layout": "IPY_MODEL_22d9b912b79a42559841edec84a970a0"
      }
     },
     "82555c4cf9ee4d02b7bac7203cd91244": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d76b7e35c5149deb29a9b45a862fb83": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e18cada6cf44c389753a4756b04aacc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "93807dc812ff4df1a0fe56dccef8022c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9627a0e9f85841acb622d3cbe9fdd6fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "  0%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8d76b7e35c5149deb29a9b45a862fb83",
       "max": 1803,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3842bccc315f4d6e87c3ef50a48eac3b",
       "value": 0
      }
     },
     "98a4080094d94fe3a19c330495a29cc5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ee82f9078cb47b3bf6126b9e69d7b1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "a298c0f1c2fd4d30a9c25e46c9df8a14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a5d21197fd5e4e49b1bc499df683f552": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a8de8db9f85e4dd1ac0bf138255c3580": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "ac1c92ea1d654e75a8b80dc0d2ec018a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Dl Completed...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_93807dc812ff4df1a0fe56dccef8022c",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a8de8db9f85e4dd1ac0bf138255c3580",
       "value": 1
      }
     },
     "ad732fd2fdda4f8db71275d9a82ad88e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b197ae12d16f479a885f8482330610a2",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_673a361778ab49938cf7b50719f135d6",
       "value": " 1/1 [00:13&lt;00:00, 13.56s/ file]"
      }
     },
     "b197ae12d16f479a885f8482330610a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b59759fd057c445187cd446224719b8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a5d21197fd5e4e49b1bc499df683f552",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_c8cf1a1159ba43558339a0d377501539",
       "value": " 0/1803 [00:00&lt;?, ? examples/s]"
      }
     },
     "b5bb4bc1c0684058b16accaab49e12a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b5f1c2e8ca804f47971def2a9a78cefb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7a009f5d170f488c8a24b6bf76027a8e",
        "IPY_MODEL_5659253e6beb422aab593f4179e00af0"
       ],
       "layout": "IPY_MODEL_6ce68526eb264938b0b5464bf91c5cc5"
      }
     },
     "b93d0233ad00446ba40f1d4f0a3a2e2d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_018ea7669b3848058732a0cafea345d3",
        "IPY_MODEL_20df8ac98b00405f8efde1904be9cf91"
       ],
       "layout": "IPY_MODEL_d92c7e1d4e344835bc7da754b6d8397d"
      }
     },
     "bafe2bb292834c0c9e01ecc45db673d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bc71e635d2434ab8823a6427345b57d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2332acd69994cf89ead0f1f51cce5c6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c258713139df4e75930edbac52937953": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "danger",
       "description": "  0%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_747ec4c3baf3496ea2b77bc4a1ed51fe",
       "max": 1193,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9ee82f9078cb47b3bf6126b9e69d7b1d",
       "value": 0
      }
     },
     "c52a1d2ae53149d383615cdacbdf02ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ac1c92ea1d654e75a8b80dc0d2ec018a",
        "IPY_MODEL_c8acf766b25642c0af3860fc437e9b41"
       ],
       "layout": "IPY_MODEL_8e18cada6cf44c389753a4756b04aacc"
      }
     },
     "c8acf766b25642c0af3860fc437e9b41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cb054f28fd6042029cdfb3f7fd49425a",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_213499c03b4940809bd72db4968473ab",
       "value": " 1/1 [00:13&lt;00:00, 13.60s/ url]"
      }
     },
     "c8cf1a1159ba43558339a0d377501539": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cb054f28fd6042029cdfb3f7fd49425a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d00227bbcdc84c38917880ffde53c000": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_133eb1b8177c4465a4301f7e6f969a7d",
        "IPY_MODEL_ad732fd2fdda4f8db71275d9a82ad88e"
       ],
       "layout": "IPY_MODEL_4f226da6604e43b3980fba7a42f7014d"
      }
     },
     "d92c7e1d4e344835bc7da754b6d8397d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eeb8316d3551414ab4f9349da201a9f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f5601b1f66a24cbd807f5af49163fa9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "f844670eef6a4f01bc38d3f883c2714e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "faee87968e434f409848bbaf5da8f553": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "faf87e478e3d48fa87a6625599a02f77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd12d91343104529b0913e56979f8b92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c258713139df4e75930edbac52937953",
        "IPY_MODEL_5fc9be90025243d4bff72c617827ad98"
       ],
       "layout": "IPY_MODEL_bc71e635d2434ab8823a6427345b57d9"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
